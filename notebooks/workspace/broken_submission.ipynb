{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a59a54",
   "metadata": {},
   "source": [
    "# Multi AI Agent Server + BigQuery Data Cleaning Stage Workflo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc6def",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "Businesses face a paradox: data is abundant but often messy, fragmented, and unstructured, which makes it hard to harness effectively. As datasets grow in scale and complexity, the same problems recur:\n",
    "\n",
    "- **Analyst bottlenecks & inefficiency**  \n",
    "  Skilled analysts spend disproportionate time on repetitive cleaning, validation, and schema reconciliation. This delays insight, prevents teams from scaling, and diverts talent from higher-value modeling.\n",
    "\n",
    "- **Inconsistent and fragile anomaly handling**  \n",
    "  Ad-hoc fixes for missing values, anomalies, and duplicates introduce bias, reduce model reliability, and make pipelines brittle under new data conditions.\n",
    "\n",
    "- **Lack of real-time, reasoning-capable frameworks**  \n",
    "  Traditional pipelines perform static transformations and cannot reason about evolving data states. Without context-aware systems that update in real time, organizations suffer delayed insights, missed opportunities, and fragile decision-making.\n",
    "\n",
    "---\n",
    "\n",
    "# Objective\n",
    "\n",
    "Gaby AI is a self-orchestrating data agent that embeds Google BigQuery as its knowledge backbone, enabling automated decision-making across data preparation, anomaly handling, and analytical workflows. By coupling reinforcement learning with in-database AI/ML functions (e.g., `AI.GENERATE`, `ML.FORECAST`), Gaby executes self-feedback learning cycles that integrate real-time insights from active pipelines and historical datasets. The agent efficiently forecasts and updates expected reward trajectories, aligning its decision policies with the evolving workflows of data teams. Leveraging BigQuery’s rapid retrieval and embedded machine learning capabilities, Gaby accelerates both business intelligence and advanced data science experimentation—delivering audit-ready results in sub-second turnaround times.\n",
    "Gaby’s design goals:\n",
    "\n",
    "- Produce data that is clean, context-aware, and auditable — suitable for high-stakes business decisions without continuous human babysitting.  \n",
    "- Learn which remediation actions actually improve downstream utility instead of relying on brittle, one-off rules.  \n",
    "- Scale incident detection and repair with traceable, reproducible workflows.\n",
    "\n",
    "### Key objectives\n",
    "\n",
    "- **Automate end-to-end cleaning:** deduplication, anomaly detection, imputation, and schema consistency.  \n",
    "- **Build a reusable knowledge framework:** encode the “belief state” of data (table + vector forms) for transparency and interoperability.  \n",
    "- **Employ reinforcement learning:** adapt dataset-specific strategies rather than using fixed rules.  \n",
    "- **Preserve past insights:** maintain a dynamic observation store to enable context-aware reasoning.  \n",
    "- **Deliver near-real-time insights:** leverage BigQuery’s streaming and query capabilities to produce sub-second to low-second responses where feasible.\n",
    "\n",
    "# Solution\n",
    "\n",
    "## 1. BigQuery as a database gatekeeper  \n",
    "BigQuery stores raw and processed data plus derived observations (anomaly flags, feature statistics, cluster assignments). Automated pipelines generate human-readable schema documentation, structured cleaning flows, and database summaries — reducing preparation time from hours to minutes or seconds.\n",
    "\n",
    "## 2. Reinforcement learning integration  \n",
    "Gaby applies decision-focused learning patterns:\n",
    "\n",
    "- **Contextual bandits** for low-latency, per-field remediation (e.g., choose between median imputation, model-based imputation, or flagging).  \n",
    "- **Episodic policy learners** for multi-step repair sequences with delayed downstream effects (e.g., dedupe → impute → transform → retrain).\n",
    "\n",
    "Policies are trained or simulated offline using *proxy rewards* (data-quality gains + quick downstream proxy checks) and are governed by safety rules and human-in-the-loop thresholds so high-risk actions require review.\n",
    "\n",
    "## 3. Self-orchestrated tasks via agents\n",
    "- **Code agents:** execute cleaning and diagnostics in sandboxed staging environments.  \n",
    "- **Reasoning LLM:** coordinates workflows and produces concise, human-readable rationales for proposed fixes.  \n",
    "- **Quantized LLMs:** compact, GGUF-format variants for housekeeping tasks and low-latency inference.\n",
    "\n",
    "## 4. Real-time reasoning & validation  \n",
    "Gaby uses BigQuery ML (clustering, forecasting, and quick retrains) and generative primitives to validate candidate fixes in staging. Actions that pass validation can be auto-committed for low-risk changes; higher-risk changes are flagged for human review. Example insights:\n",
    "\n",
    "- “Sales dropped 8% in Region Y last quarter.”  \n",
    "- “Inventory for Product Z is below forecast demand.”  \n",
    "- “Customer Segment B is trending upward in lifetime value.”\n",
    "\n",
    "Gaby AI is a self-orchestrating data agent that embeds Google BigQuery as its in-database knowledge backbone, enabling automated decision-making across data preparation, anomaly handling, and analytics workflows. By coupling reinforcement-learning policies with BigQuery’s native AI/ML primitives (e.g., `AI.GENERATE`, `ML.FORECAST`) the agent runs closed-loop self-feedback cycles: it proposes fixes, validates them with fast in-database checks, observes proxy downstream effects, and updates expected reward trajectories so its policies stay aligned with evolving data-team objectives. Because the belief-state, audit logs, policy checkpoints, and validation runs all live in BigQuery, Gaby delivers auditable, reproducible recommendations with near-real-time responsiveness — often achieving sub-second to low-second turnaround for diagnostics and low-risk fixes.\n",
    "\n",
    "**Why this matters**\n",
    "- **End-to-end automation:** replaces repetitive cleaning and validation work with safe, policy-driven automation so analysts can focus on modeling and insight.  \n",
    "- **Adaptive, evidence-based decisions:** RL-driven policies learn which repairs actually improve downstream utility (not just reduce missingness), improving over time with observed outcomes.  \n",
    "- **Speed & scale:** colocated reasoning and fast retrieval reduce ETL overhead and shrink time-to-insight for both BI dashboards and data-science experiments.  \n",
    "- **Auditability & governance:** every suggested change includes a human-readable rationale, pre/post snapshots, and a recorded reward — enabling traceable, reviewable data fixes.  \n",
    "- **Practical, measurable outcomes:** design experiments to demonstrate reductions in manual review rates, improvements in downstream model metrics (AUC/RMSE), and shorter ingestion→analytics latency.\n",
    "\n",
    "**Evaluation metrics**\n",
    "- % reduction in human-reviewed fixes (automation rate)  \n",
    "- Δ downstream model performance (AUC / RMSE) after automated cleaning vs baseline  \n",
    "- Mean time to first validated insight (seconds)  \n",
    "- % of actions auto-committed vs flagged for review\n",
    "\n",
    "Gaby’s colocated, learning-first approach therefore reduces analyst toil, improves model reliability, and accelerates safe, reproducible decision-making across production analytics and data-science workflows.\n",
    "\n",
    "---\n",
    "\n",
    "# Links & demo\n",
    "\n",
    "- **Demo:** http://gaby.mimeus.com — *fallback:* Hugging Face demo: https://mimipynb-agent-sandbox.hf.space/  \n",
    "  Dump any unclean dataset and watch Gaby do it all for you!\n",
    "\n",
    "- **GitHub:** https://github.com/whoamimi/bigquery-comp  \n",
    "- **Blog:** http://github.com/whoamimi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bba23b",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "\n",
    "- [Architecture](#architecture)\n",
    "    - [Reinforcement Learning Architecture](#reinforcement-learning-architecture)\n",
    "- [Setup Workspace](#setup-workspace)\n",
    "    - [Project Configuration & BigQuery Setup](#project-configuration--bigquery-setup)\n",
    "- [Prompts](#prompts)\n",
    "    - [Episode Window Prompt Template](#episode-window-prompt-template)\n",
    "- [Stage I: Data Cleaning](#stage-i-data-cleaning)\n",
    "    - [Helper Functions](#helper-functions)\n",
    "    - [Data Schemas](#data-schemas)\n",
    "    - [Gaby Main Agent Workflow](#gaby-main-agent-workflow)\n",
    "    - [BigQuery Gatekeeper](#bigquery-gatekeeper)\n",
    "    - [Data Cleaning Pipeline Execution](#data-cleaning-pipeline-execution)\n",
    "    - [Final Report](#final-report)\n",
    "- [Stage II: Missing Data Values](#stage-ii-missing-data-values)\n",
    "    - [Missing Data Pattern Analysis](#missing-data-pattern-analysis)\n",
    "- [Agent Reinforcement Learning & LifeCycle](#agent-reinforcement-learning--lifecycle)\n",
    "    - [Recent Observations Fetching](#recent-observations-fetching)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ddcb2",
   "metadata": {},
   "source": [
    "## Setup Workspace\n",
    "\n",
    "This section initializes the development environment, imports required libraries, establishes BigQuery connections, and configures project-specific parameters for the Gaby AI data cleaning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e566b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6e700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imported GabyBasement & Instructor from gaby_agent.core.agent._core\n"
     ]
    }
   ],
   "source": [
    "# Project path & core agent imports\n",
    "# This cell sets up the Python path to import Gaby AI's core components\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Define project root explicitly (adjust if repo moved)\n",
    "PROJECT_ROOT = Path('/Users/mimiphan/mimeus-app/backend/gaby')\n",
    "SRC_PATH = PROJECT_ROOT / 'src'\n",
    "if str(SRC_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_PATH))\n",
    "\n",
    "try:\n",
    "    from gaby_agent.core.agent._core import GabyBasement, Instructor\n",
    "    print('✅ Imported GabyBasement & Instructor from gaby_agent.core.agent._core')\n",
    "except ModuleNotFoundError as e:\n",
    "    print('❌ Import failed. Check that SRC_PATH is correct and package __init__ files exist.')\n",
    "    print(e)\n",
    "except Exception as e:\n",
    "    print('Unexpected error during import:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c510ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbd284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery Configuration - Project, Dataset, and Model Setup\n",
    "# This cell defines all the BigQuery resources used throughout the notebook\n",
    "\n",
    "# CURRENT TESTING CONFIGURATION\n",
    "BQ_PROJECT_ID = client.project\n",
    "BQ_DATASET_ID = \"cleaning_service\"  # Database to store unclean data and workspace\n",
    "BQ_TABLE_ID = \"sample_dataset\"      # actual data table\n",
    "SAMPLE_FULL_ID = \"genial-motif-472804-s1.cleaning_service.sample_dataset\"\n",
    "SAMPLE_SUMMARY_FULL_ID = \"genial-motif-472804-s1.cleaning_service.field_summary\"\n",
    "\n",
    "# PRODUCTION DATABASE CONFIGURATION\n",
    "# Dataset organization for different data types\n",
    "BQ_DATASET_OBSERVATION_ID = \"observations\"  # Stores agent observations and learning data\n",
    "BQ_DATASET_ACTION_ID = \"cognitive\"          # Stores agent decision patterns\n",
    "BQ_DATASET_OUTPUT_ID = \"cleaned_data\"       # Stores final cleaned datasets\n",
    "\n",
    "# Table naming conventions\n",
    "BQ_TABLE_SUMMARY_ID = \"field_summary\"  # Summary tables prefixed with episode_id\n",
    "\n",
    "# GENERATIVE AI MODEL CONFIGURATION\n",
    "# BigQuery ML remote model configuration for Gemini\n",
    "BQ_MODEL_CONNECTION = \"projects/481034637222/locations/australia-southeast1/connections/__default_cloudresource_connection__\"\n",
    "BQ_MODEL_ENDPOINT = \"projects/genial-motif-472804-s1/locations/australia-southeast1/publishers/google/models/gemini-2.5-flash\"\n",
    "BQ_MODEL_ID = \"genial-motif-472804-s1.cleaning_service.gatekeeper\"\n",
    "DEFAULT_MODEL_TYPE = \"gemini-2.5-flash\"\n",
    "\n",
    "# LOCAL DEVELOPMENT FILES\n",
    "LOCAL_SAMPLE_PATH = \"/Users/mimiphan/mimeus-app/backend/gaby/src/gaby_agent/data/input/dirty_cafe_sales.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d9dec",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "This section defines the prompt templates used by Gaby AI for decision-making and data analysis. These prompts guide the agent's reasoning process during data cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episode Window Prompt Template\n",
    "# This prompt guides the agent's decision-making process during data cleaning episodes\n",
    "EPISODE_WINDOW_PROMPT = \"\"\"\n",
    "You are a senior data analyst and currently performing data cleaning tasks.\n",
    "{current_workspace}\n",
    "Given the following knowledge of your current dataset, respond with your next action from one of the following options:\n",
    "{action_space}\n",
    "Your response must be one of the action options.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1cd490",
   "metadata": {},
   "source": [
    "## Stage I: Data Cleaning\n",
    "\n",
    "This is the primary data cleaning stage where Gaby AI analyzes datasets, generates field descriptions, and performs initial data quality assessment. The stage integrates local Python processing with BigQuery's generative AI capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ae4fc",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "This subsection contains utility functions and decorators that facilitate data processing, BigQuery interactions, and DataFrame operations throughout the cleaning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97fbe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorator for BigQuery SQL Execution\n",
    "# This decorator automatically executes SQL queries and returns pandas DataFrames\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "def pandas_gatekeeper(func):\n",
    "    \"\"\"\n",
    "    A decorator that executes a SQL query generated by a function and returns a DataFrame.\n",
    "\n",
    "    This decorator intercepts the SQL string returned by the wrapped function,\n",
    "    executes it using the provided BigQuery client, and returns the result\n",
    "    as a pandas DataFrame.\n",
    "\n",
    "    The decorated function must be called with a 'client' keyword argument\n",
    "    of type `google.cloud.bigquery.Client`.\n",
    "\n",
    "    Args:\n",
    "        func: The function to be decorated, which should return a SQL query string.\n",
    "\n",
    "    Returns:\n",
    "        A wrapper function that executes the query and returns a DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If the 'client' keyword argument is not provided or is not a\n",
    "                   `bigquery.Client` instance.\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "\n",
    "        # client = bigquery.Client() -- Since running in notebook.\n",
    "\n",
    "        # Call the original function to get the SQL query string\n",
    "        sql_query = func(*args, **kwargs)\n",
    "\n",
    "        print(f\"--- Executing SQL from '{func.__name__}' ---\")\n",
    "        print(sql_query)\n",
    "\n",
    "        # Execute the query and return the result as a pandas DataFrame\n",
    "        job = client.query(sql_query)\n",
    "        return job.to_dataframe()\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS FOR DATA ANALYSIS AND BIGQUERY OPERATIONS\n",
    "\n",
    "def summarize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates summary statistics for each column in a DataFrame.\n",
    "\n",
    "    For each column, calculates:\n",
    "    - Missing value count and percentage\n",
    "    - Data type information\n",
    "    - Unique value count (or \"continuous\" for numeric columns with >20 unique values)\n",
    "\n",
    "    Args:\n",
    "        df: Input pandas DataFrame to analyze\n",
    "\n",
    "    Yields:\n",
    "        dict: Summary statistics for each column\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        total_count = len(df)\n",
    "        missing_count = df[col].isna().sum()\n",
    "        data_type = df[col].dtype\n",
    "\n",
    "        # Check if continuous: numeric with many unique values\n",
    "        if pd.api.types.is_numeric_dtype(df[col]) and df[col].nunique() > 20:\n",
    "            unique_vals = \"continuous\"\n",
    "        else:\n",
    "            unique_vals = df[col].nunique()\n",
    "\n",
    "        yield {\n",
    "            \"data_field_name\": col,\n",
    "            \"missing_count\": missing_count,\n",
    "            \"total_count\": total_count,\n",
    "            \"data_type\": str(data_type),\n",
    "            \"unique_values\": unique_vals\n",
    "        }\n",
    "\n",
    "\n",
    "def upload_dataframe_to_bq(df: pd.DataFrame, project_id: str, dataset_id: str, table_id: str):\n",
    "    \"\"\"\n",
    "    Uploads a pandas DataFrame to a specified BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to upload\n",
    "        project_id: BigQuery project ID\n",
    "        dataset_id: BigQuery dataset ID\n",
    "        table_id: BigQuery table ID\n",
    "    \"\"\"\n",
    "\n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        autodetect=True\n",
    "    )\n",
    "    job = client.load_table_from_dataframe(\n",
    "        df, table_ref, job_config=job_config)\n",
    "    job.result()  # Wait for the job to complete.\n",
    "    print(f\"✅ Data uploaded to {table_ref}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396dded",
   "metadata": {},
   "source": [
    "### Data Schemas \n",
    "\n",
    "This subsection defines the data structures and configuration classes used throughout the data cleaning pipeline. These schemas ensure consistent data handling and episode tracking across the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1cd490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reports & Profilers for updating databases in BigQuery between sessions etc.\n",
    "\n",
    "# PROFILERS\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EpisodeConfig:\n",
    "    input_id: str = SAMPLE_FULL_ID\n",
    "    summary_id: str = SAMPLE_SUMMARY_FULL_ID\n",
    "    # BIGQUERY MODEL STORED LOCS\n",
    "    bq_model_connection: str = BQ_MODEL_CONNECTION\n",
    "    bq_model_endpoint: str = BQ_MODEL_ENDPOINT\n",
    "    bq_model_id: str = BQ_MODEL_ID\n",
    "    default_model_type: str = DEFAULT_MODEL_TYPE\n",
    "    # DEFAULT NAMES\n",
    "    default_project_id: str = BQ_PROJECT_ID\n",
    "    default_dataset_id: str = BQ_DATASET_ID\n",
    "    default_table_id: str = BQ_TABLE_ID\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EntryReport:\n",
    "    description: str | None = None\n",
    "    data_field_summary: pd.DataFrame | None = None\n",
    "    data_field_description: pd.DataFrame | None = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MissingDataReport:\n",
    "    missing_pattern: str\n",
    "    missing_count: float\n",
    "    missing_perc: float\n",
    "    data_field_type: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataProfiler:\n",
    "    # User Inputs\n",
    "    data: pd.DataFrame\n",
    "    user_input_tags: str | list | None = None\n",
    "\n",
    "    # Defining the dataset\n",
    "    description: str | None = None  # Describing the dataset in natural language\n",
    "    data_field_summary: pd.DataFrame | None = None  # Data field summary table\n",
    "    # Data field summary table with description columns\n",
    "    data_field_description: pd.DataFrame | None = None\n",
    "    _send_to_gatekeeper: bool = False\n",
    "\n",
    "    # Episode ID & Configuration\n",
    "    episode_id: str = uuid4().hex\n",
    "    timestamp: str = datetime.now().isoformat()\n",
    "    config: EpisodeConfig | None = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.define_dataset(self._send_to_gatekeeper)\n",
    "        # the EpisodeConfig stored dataset id for the input dataset is set to self.episode_id-self.tiemstamp in prod\n",
    "        # self.config = EpisodeConfig(\n",
    "        #    input_dataset_id=f\"{self.episode_id}-{self.timestamp}\",\n",
    "        #    summary_table_id=f\"{self.episode_id}-{BQ_TABLE_SUMMARY_ID}\"\n",
    "        # )\n",
    "        self.config = EpisodeConfig()\n",
    "\n",
    "    def define_dataset(self, upload_summary: bool = False):\n",
    "        if self.data.shape[0] == 0 or self.user_input_tags is None:\n",
    "            raise ValueError(\n",
    "                \"The provided DataFrame is empty or data origin is not specified. Both these are required to start the workflow.\")\n",
    "\n",
    "        # assuming have loaded the model and returned it\n",
    "        self.data_field_summary = pd.DataFrame.from_records(\n",
    "            list(summarize_dataframe(self.data)))\n",
    "\n",
    "        print(\n",
    "            f\"✅ Dataset defined with {self.data.shape[0]} rows and {self.data.shape[1]} columns.\")\n",
    "\n",
    "        if upload_summary is True:\n",
    "            upload_dataframe_to_bq(\n",
    "                self.data, BQ_PROJECT_ID, BQ_DATASET_ID, BQ_TABLE_ID)\n",
    "            upload_dataframe_to_bq(\n",
    "                self.data_field_summary, BQ_PROJECT_ID, BQ_DATASET_ID, BQ_TABLE_SUMMARY_ID)\n",
    "\n",
    "        print(\n",
    "            f\"Completed profiling for dataset id: {self.episode_id} and uploaded to BQ.\")\n",
    "\n",
    "    @property\n",
    "    def end_cleaning_report(self) -> EntryReport:\n",
    "        return EntryReport(\n",
    "            description=self.description,\n",
    "            data_field_summary=self.data_field_summary,\n",
    "            data_field_description=self.data_field_description\n",
    "        )\n",
    "\n",
    "    def episode_recap(self):\n",
    "        context_prompt = f\"--- Data Summary ---\"\n",
    "\n",
    "        end_cleaning_report = self.end_cleaning_report\n",
    "\n",
    "        context_prompt += f\"\\nDataset Description: {end_cleaning_report.description}\\n\" if end_cleaning_report.description is not None else \"\\nNo dataset description available.\\n\"\n",
    "        context_prompt += f\"\\nData Field Summary:\\n{end_cleaning_report.data_field_summary.to_markdown(index=False)}\\n\" if end_cleaning_report.data_field_summary is not None else \"\"\n",
    "        context_prompt += f\"\\nData Field Description:\\n{end_cleaning_report.data_field_description.to_markdown(index=False)}\\n\" if end_cleaning_report.data_field_description is not None else \"\"\n",
    "\n",
    "        return context_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2920b4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset defined with 10000 rows and 8 columns.\n",
      "Completed profiling for dataset id: 8fcb0efffe114abe8ec09730c159829c and uploaded to BQ.\n"
     ]
    }
   ],
   "source": [
    "# load the datasets into workspace\n",
    "data = pd.read_csv(LOCAL_SAMPLE_PATH, header=0)\n",
    "gb = DataProfiler(data=data, user_input_tags=[\"sales\", \"beverages\", \"cafe\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884c650",
   "metadata": {},
   "source": [
    "### Gaby Main Agent Workflow\n",
    "\n",
    "This subsection implements the core AI agents responsible for dataset analysis and field description generation. These agents use the GabyBasement framework with structured prompts to analyze data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162cf538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" clean_stage_a.py \"\"\"\n",
    "\n",
    "\n",
    "class DatasetSummarizer(\n",
    "    GabyBasement,\n",
    "    prompt=Instructor(\n",
    "        prompt=\"\"\"\n",
    "        You are a senior data analyst. Based on the dataset’s fields and descriptive metadata, provide a concise summary (no more than 2 sentences) that highlights:\n",
    "        •\tthe dataset’s key characteristics and notable features or patterns,\n",
    "        •\tpotential modeling or analytical objectives it may support, and\n",
    "        •\twhether the dataset contains any unique identifiers.\n",
    "        \"\"\",\n",
    "        input_template=\"\"\"\n",
    "        Dataset descriptive labels: {user_inputs},\n",
    "        Dataset Subset:\n",
    "        {data_table}\n",
    "        \"\"\"\n",
    "    )\n",
    "):\n",
    "    pass\n",
    "\n",
    "\n",
    "class DataFieldMetaDescription(\n",
    "    GabyBasement,\n",
    "    prompt=Instructor(\n",
    "        prompt=\"You are a data analyst. Given the dataset description and a specific data field label, return a concise description of what the data field possibly means in the context of the dataset. Return your response in at most 1 sentence.\",\n",
    "        input_template=\"\"\"\n",
    "        Dataset Description: {data_description}\n",
    "        Data Field Label: {data_label}\n",
    "        Data Sample: {data_sample}\n",
    "        \"\"\"\n",
    "    )\n",
    "):\n",
    "    def run_loop(self, data: pd.DataFrame, data_description: str) -> dict:\n",
    "        \"\"\" Run the description for each data field in the dataframe. \"\"\"\n",
    "\n",
    "        descriptions = {}\n",
    "\n",
    "        for column in data.columns:\n",
    "            sample = data[column].dropna().unique()[:3].tolist()\n",
    "            sample_str = \", \".join(map(str, sample))\n",
    "\n",
    "            description = self.run(\n",
    "                data_description=data_description,\n",
    "                data_label=column,\n",
    "                data_sample=sample_str\n",
    "            )\n",
    "\n",
    "            descriptions[column] = [\n",
    "                {\n",
    "                    'name': self.post_process(description),\n",
    "                    'data_type': str(data[column].dtype),\n",
    "                    'description': description\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899eebae",
   "metadata": {},
   "source": [
    "### BigQuery Gatekeeper \n",
    "\n",
    "This subsection implements the BigQuery-based AI functions that leverage Google's generative AI models for data field analysis. It demonstrates direct integration with BigQuery's `AI.GENERATE` function for scalable data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_DESCRIBE_DATA_FIELD_LABEL = \"\"\"\n",
    "SELECT\n",
    "  data_field_name,\n",
    "  AI.GENERATE( ('The data field name ',\n",
    "      data_field_name,\n",
    "      'with values of data type,',\n",
    "      data_type,\n",
    "      'is one of the dataset column labels of a dataset with description: cafe sale logs. In a sentence, define what each data field represent.'),\n",
    "    connection_id => '{connection_id}',\n",
    "    endpoint => '{endpoint}',\n",
    "    output_schema => 'data_field_name STRING, description STRING').description\n",
    "FROM\n",
    "  {data_summary_id};\n",
    "\"\"\"\n",
    "\n",
    "# 2 methods in this file to distinguish numeric values.\n",
    "# Ordinal: categories have a natural order but no consistent distance between them (e.g., low, medium, high)\n",
    "SQL_DETECT_NUMERIC_FIELD = \"\"\"\n",
    "SELECT\n",
    "  data_field_name,\n",
    "  AI.GENERATE( ('The data field name ',\n",
    "      data_field_name,\n",
    "      'with values of data type,',\n",
    "      data_type,\n",
    "      'is one of the dataset column labels of a dataset with description: cafe sale logs.'\n",
    "      'Classify the dataset field into one of: Nominal, Ordinal, Continuous, Unknown, if the data type is numerical and if not, return Unknown. Return the result strictly as: {\"data_type\": \"<Nominal|Ordinal|Continuous|Unknown>\"}'\n",
    "      ),\n",
    "    connection_id => '{connection_id}',\n",
    "    endpoint => '{endpoint}',\n",
    "    output_schema => 'data_field_name STRING, numeric_type STRING').numeric_type\n",
    "FROM\n",
    "  {data_summary_id};\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@pandas_gatekeeper\n",
    "def describe_data_field(\n",
    "    data_summary_id: str,\n",
    "    connection_id: str = BQ_MODEL_CONNECTION,\n",
    "    endpoint: str = DEFAULT_MODEL_TYPE\n",
    "):\n",
    "    return SQL_DESCRIBE_DATA_FIELD_LABEL.format(\n",
    "        data_summary_id=data_summary_id,\n",
    "        connection_id=connection_id,\n",
    "        endpoint=endpoint\n",
    "    )\n",
    "\n",
    "\n",
    "@pandas_gatekeeper\n",
    "def detect_numeric_field(\n",
    "    data_summary_id: str,\n",
    "    connection_id: str = BQ_MODEL_CONNECTION,\n",
    "    endpoint: str = DEFAULT_MODEL_TYPE\n",
    "):\n",
    "    return SQL_DETECT_NUMERIC_FIELD.format(\n",
    "        data_summary_id=data_summary_id,\n",
    "        connection_id=connection_id,\n",
    "        endpoint=endpoint\n",
    "    )\n",
    "\n",
    "# gb.data_field_description = describe_data_field(\n",
    "#    data_summary_id=SAMPLE_SUMMARY_FULL_ID,\n",
    "#    connection_id=BQ_MODEL_CONNECTION,\n",
    "#    endpoint=DEFAULT_MODEL_TYPE\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac32c0",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "This subsection orchestrates the complete data cleaning pipeline, combining local processing with BigQuery AI capabilities. It demonstrates fallback mechanisms when cloud resources are unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning_pipeline(report: DataProfiler) -> DataProfiler:\n",
    "    \"\"\" Main function to run the data cleaning pipeline. \"\"\"\n",
    "\n",
    "    report.description = DatasetSummarizer().run(\n",
    "        user_inputs=report.user_input,\n",
    "        data_table=report.data.head(3).to_string(index=False)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        report.data_field_description = describe_data_field(\n",
    "            connection_id=report.config.bq_model_connection,\n",
    "            model_endpoint=report.config.default_model_type,\n",
    "        )\n",
    "\n",
    "        report.numeric_table = detect_numeric_field(\n",
    "            connection_id=report.config.bq_model_connection,\n",
    "            endpoint=report.config.default_model_type,\n",
    "            data_summary_id=report.config.summary_id\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Error using GCP model, falling back to local model (takes longer to run):\", e)\n",
    "\n",
    "        report.data_field_description = DataFieldMetaDescription().run_loop(\n",
    "            data=report.data,\n",
    "            data_description=report.description\n",
    "        )\n",
    "        report.numeric_field = None\n",
    "    return report\n",
    "\n",
    "\n",
    "gb = data_cleaning_pipeline(gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063aa0ff",
   "metadata": {},
   "source": [
    "### Final Report \n",
    "\n",
    "This subsection displays the results of the data cleaning pipeline. It shows the generated dataset summaries, field descriptions, and data quality assessments produced by Gaby AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d15f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_field_name</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>data_type</th>\n",
       "      <th>unique_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transaction ID</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item</td>\n",
       "      <td>333</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quantity</td>\n",
       "      <td>138</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Price Per Unit</td>\n",
       "      <td>179</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total Spent</td>\n",
       "      <td>173</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Payment Method</td>\n",
       "      <td>2579</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Location</td>\n",
       "      <td>3265</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transaction Date</td>\n",
       "      <td>159</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_field_name  missing_count  total_count data_type  unique_values\n",
       "0    Transaction ID              0        10000    object          10000\n",
       "1              Item            333        10000    object             10\n",
       "2          Quantity            138        10000    object              7\n",
       "3    Price Per Unit            179        10000    object              8\n",
       "4       Total Spent            173        10000    object             19\n",
       "5    Payment Method           2579        10000    object              5\n",
       "6          Location           3265        10000    object              4\n",
       "7  Transaction Date            159        10000    object            367"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.data_field_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be81c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the dataset in 2 sentences:\n",
      "\n",
      "This dataset appears to track sales transactions from a cafe, capturing details such as items purchased, quantities, prices per unit, and total spent at each location. The data contains some inconsistencies due to missing values or unknown payment methods, but overall suggests that the cafe sells beverages like coffee and cake, with customers often purchasing in-store.\n"
     ]
    }
   ],
   "source": [
    "print(gb.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935b8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_field_name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Spent</td>\n",
       "      <td>This field represents the total amount of mone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Payment Method</td>\n",
       "      <td>This field represents the method of payment us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Item</td>\n",
       "      <td>The 'Item' field represents the name of the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Location</td>\n",
       "      <td>This field represents the geographical locatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantity</td>\n",
       "      <td>Represents the number of items sold in a cafe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transaction ID</td>\n",
       "      <td>cafe sale logs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Price Per Unit</td>\n",
       "      <td>This field represents the cost of a single ite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transaction Date</td>\n",
       "      <td>This data field represents the date of a trans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_field_name                                        description\n",
       "0       Total Spent  This field represents the total amount of mone...\n",
       "1    Payment Method  This field represents the method of payment us...\n",
       "2              Item  The 'Item' field represents the name of the pr...\n",
       "3          Location  This field represents the geographical locatio...\n",
       "4          Quantity  Represents the number of items sold in a cafe ...\n",
       "5    Transaction ID                                     cafe sale logs\n",
       "6    Price Per Unit  This field represents the cost of a single ite...\n",
       "7  Transaction Date  This data field represents the date of a trans..."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.data_field_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3982947",
   "metadata": {},
   "source": [
    "## Stage II: Missing Data Values\n",
    "\n",
    "This stage focuses on analyzing missing data patterns and recommending appropriate handling strategies. It demonstrates advanced BigQuery AI capabilities for context-aware missing data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88167ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_MISSING_DATA_PATTERN_METHOD = \"\"\"\n",
    "SELECT\n",
    "  data_field_name,\n",
    "  data_type,\n",
    "  AI.GENERATE( ('The data field name ',\n",
    "      data_field_name,\n",
    "      'is sourced from dataset with description: car sale logs.',\n",
    "\n",
    "      'Data field is of data type,',\n",
    "      data_type,\n",
    "      'and has a total',\n",
    "      missing_count,\n",
    "      'number of missing values. Suggest the most common method in dealing with missing datasets of this data type and context. Your response must only contain one of the values fromthe following: [\\'imputation\\', \\'drop_missing\\', ].'),\n",
    "    connection_id => '{connection_id}',\n",
    "    endpoint => '{endpoint}',\n",
    "    output_schema => 'data_field_name STRING, description STRING').description\n",
    "FROM\n",
    "  {data_summary_id};\n",
    "\"\"\"\n",
    "\n",
    "@pandas_gatekeeper\n",
    "def suggest_missing_data_pattern_method(\n",
    "    data_summary_id: str,\n",
    "    connection_id: str = BQ_MODEL_CONNECTION,\n",
    "    endpoint: str = DEFAULT_MODEL_TYPE\n",
    "):\n",
    "    return SQL_MISSING_DATA_PATTERN_METHOD.format(\n",
    "        data_summary_id=data_summary_id,\n",
    "        connection_id=connection_id,\n",
    "        endpoint=endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767753b",
   "metadata": {},
   "source": [
    "### Helper Functions & Utils\n",
    "\n",
    "This subsection contains specialized functions for missing data pattern analysis and recommendation generation using BigQuery's generative AI capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28797e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_MISSING_DATA_PATTERN_METHOD = \"\"\"\n",
    "SELECT\n",
    "  data_field_name,\n",
    "  data_type,\n",
    "  AI.GENERATE( ('The data field name ',\n",
    "      data_field_name,\n",
    "      'is sourced from dataset with description: car sale logs.',\n",
    "\n",
    "      'Data field is of data type,',\n",
    "      data_type,\n",
    "      'and has a total',\n",
    "      missing_count,\n",
    "      'number of missing values. Suggest the most common method in dealing with missing datasets of this data type and context. Your response must only contain one of the values fromthe following: [\\'imputation\\', \\'drop_missing\\', ].'),\n",
    "    connection_id => '{connection_id}',\n",
    "    endpoint => '{endpoint}',\n",
    "    output_schema => 'data_field_name STRING, description STRING').description\n",
    "FROM\n",
    "  {data_summary_id};\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55723b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_missing_data_pattern(\n",
    "    client: bigquery.Client,\n",
    "    summary_table_id: str,\n",
    "    model_id: str,\n",
    "    data_context: str = \"general business data\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function only runs when there have been no previous observations related to similar datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # This query constructs a prompt for each row and calls the model.\n",
    "    # It returns the original field name alongside the generated description.\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "      data_field_name,\n",
    "      ai_generate_table_result AS description\n",
    "    FROM\n",
    "      AI.GENERATE_TABLE(\n",
    "        MODEL `{model_id}`,\n",
    "        (\n",
    "          SELECT\n",
    "            data_field_name,\n",
    "            CONCAT(\n",
    "              'Given a dataset about {data_context}, ',\n",
    "              'provide a one-sentence business description for a data field named: ',\n",
    "              data_field_name\n",
    "            ) AS prompt\n",
    "          FROM\n",
    "            `{summary_table_id}`\n",
    "        ),\n",
    "        STRUCT(\n",
    "          0.2 AS temperature,\n",
    "          20 AS max_output_tokens\n",
    "        )\n",
    "      );\n",
    "    \"\"\"\n",
    "    print(\"--- Running Query to Generate Descriptions ---\")\n",
    "    print(query)\n",
    "\n",
    "    # Execute the query and return the results as a DataFrame\n",
    "    job = client.query(query)\n",
    "    return job.to_dataframe()\n",
    "\n",
    "\n",
    "data_field_descriptions_take_2 = generate_descriptions_for_fields(\n",
    "    client=client,\n",
    "    summary_table_id=SAMPLE_SUMMARY_FULL_ID,\n",
    "    model_id=BQ_MODEL_ID,\n",
    "    data_context=\"cafe sales data including beverages and food items\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25653444",
   "metadata": {},
   "source": [
    "#### Stage 3: Detect Anomalities \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5eea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_fields(df: pd.DataFrame,\n",
    "                          numeric_types=(\"INT64\", \"FLOAT64\", \"NUMERIC\"),\n",
    "                          min_unique=20,\n",
    "                          min_unique_ratio=0.05):\n",
    "    \"\"\"\n",
    "    Return only the continuous fields from a dataset summary DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Must have columns [data_field_name, data_type, unique_values, total_count].\n",
    "        numeric_types (tuple): BigQuery numeric types considered numeric.\n",
    "        min_unique (int): Minimum unique values threshold.\n",
    "        min_unique_ratio (float): Minimum ratio (unique / total_count).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing only continuous fields.\n",
    "    \"\"\"\n",
    "    mask = (\n",
    "        df[\"data_type\"].isin(numeric_types)\n",
    "        & (\n",
    "            (df[\"unique_values\"] > min_unique) |\n",
    "            (df[\"unique_values\"] / df[\"total_count\"] > min_unique_ratio)\n",
    "        )\n",
    "        & (df[\"unique_values\"] < df[\"total_count\"])  # exclude pure IDs\n",
    "    )\n",
    "    return df.loc[mask].reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
