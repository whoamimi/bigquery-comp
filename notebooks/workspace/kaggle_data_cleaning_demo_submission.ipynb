{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81948d9b",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Self-orchestrated Data Cleaning AI Agent](#toc0_)\n",
    "\n",
    "This notebook demonstrates how to use **Google BigQuery AI/ML** as AI Agent's Memory Gatekeeper. This AI Agent is called Gaby and he self-orchestrates his decision-making processes via Supervised Multi-Agent Architecture ([source](https://collabnix.com/multi-agent-and-multi-llm-architecture-complete-guide-for-2025/)). BigQuery ML methods allows Gaby to recall previous observations in real time - and not just rely on chat session's caches - for his decision-making processes for example, estimating reward states using `AI.FORECAST`. \n",
    "\n",
    "Reward states are always relative to the user / data team members and are weighed against insights from his observing states i.e. data cleaning sessions.\n",
    "\n",
    "This notebook highlights the stages of building a robust data cleaning workflow with BigQuery AI/ML and self-orchestrated agents. \n",
    "\n",
    "Data Cleaning Stages include:\n",
    "\n",
    "1. Data Documentation and Summary\n",
    "2. Strategies in handling Missing Values\n",
    "3. Handling Anomalities \n",
    "4. Reinforcement Learning Strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e8523e",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "\n",
    "- [Setup Workspace](#toc1_1_)    \n",
    "  - [Core Build](#toc1_1_1_)    \n",
    "    - [Prompts](#toc1_1_1_1_)    \n",
    "    - [Ollama Wrapper](#toc1_1_1_2_)    \n",
    "    - [Helper Functions](#toc1_1_1_3_)    \n",
    "    - [Data Schemas](#toc1_1_1_4_)    \n",
    "  - [Stage I: Data Documentation and Reporting](#toc1_1_2_)    \n",
    "    - [Self-Orchestrating Prompt Chain with Ollama](#toc1_1_2_1_)    \n",
    "    - [BigQuery Gatekeeper](#toc1_1_2_2_)    \n",
    "    - [To Run Workflow](#toc1_1_2_3_)    \n",
    "    - [Final Report](#toc1_1_2_4_)    \n",
    "  - [Stage II: Missing Data Values](#toc1_1_3_)    \n",
    "    - [Helper Functions & Utils](#toc1_1_3_1_)    \n",
    "  - [Stage III: Data Anomality Detection For Continous Dataset](#toc1_1_4_)    \n",
    "- [Agent Reinforcement Learning & LifeCycle](#toc1_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ddcb2",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Setup Workspace](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e566b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbd284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN TESTING -- CURRENTLY USED IN THIS WORKSPACE\n",
    "BQ_PROJECT_ID = client.project\n",
    "BQ_DATASET_ID = \"cleaning_service\" # Database to store unclean and workspace for now\n",
    "BQ_TABLE_ID = \"sample_dataset\" # actual data table named as\n",
    "SAMPLE_FULL_ID = \"genial-motif-472804-s1.cleaning_service.sample_dataset\"\n",
    "SAMPLE_SUMMARY_FULL_ID = \"genial-motif-472804-s1.cleaning_service.field_summary\"\n",
    "\n",
    "# TRUE DB CONFIGURATION\n",
    "# DATASET NAMES\n",
    "BQ_DATASET_OBSERVATION_ID = \"observations\"\n",
    "BQ_DATASET_ACTION_ID = \"cognitive\"\n",
    "BQ_DATASET_OUTPUT_ID = \"cleaned_data\"\n",
    "\n",
    "# TABLE NAMES\n",
    "BQ_TABLE_SUMMARY_ID = \"field_summary\" # stores in the summary of the uploaded dataset should be prefixed with the episodes id\n",
    "\n",
    "# GATEKEEPER GENERATIVE MODEL\n",
    "BQ_MODEL_CONNECTION = \"projects/481034637222/locations/australia-southeast1/connections/__default_cloudresource_connection__\"\n",
    "BQ_MODEL_ENDPOINT = \"projects/genial-motif-472804-s1/locations/australia-southeast1/publishers/google/models/gemini-2.5-flash\"\n",
    "BQ_MODEL_ID = \"genial-motif-472804-s1.cleaning_service.gatekeeper\"\n",
    "DEFAULT_MODEL_TYPE = \"gemini-2.5-flash\"\n",
    "\n",
    "# LOCAL FILE PATHS\n",
    "LOCAL_SAMPLE_PATH = \"/Users/mimiphan/mimeus-app/backend/gaby/src/gaby_agent/data/input/dirty_cafe_sales.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b826c",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[Core Build](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d9dec",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_1_'></a>[Prompts](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_WINDOW_PROMPT = \"\"\"\n",
    "You are a senior data analyst and currently performing data cleaning tasks.\n",
    "{current_workspace}\n",
    "Given the following knowledge of your current dataset, respond with your next action from one of the following options:\n",
    "{action_space}\n",
    "Your response must be one of the action options.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6753e07",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_2_'></a>[Ollama Wrapper](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65691232",
   "metadata": {},
   "source": [
    "Ollama Wrapper as a fallback method incase Kaggle does not support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6435a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "src.gaby_agent.core.agent._core\n",
    "Core classes and functions for the Gaby Agent system.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from abc import ABC\n",
    "from pathlib import Path\n",
    "from threading import Lock\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass\n",
    "\n",
    "try:\n",
    "    os.chdir(Path(__file__).resolve().parents[4])\n",
    "except Exception:\n",
    "    os.chdir(Path.cwd().root if hasattr(Path.cwd(), \"root\") else Path.cwd())\n",
    "\n",
    "load_dotenv(\".env.local\")\n",
    "\n",
    "import ollama\n",
    "\n",
    "@dataclass\n",
    "class Instructor:\n",
    "    prompt: str\n",
    "    input_template: str | None = None\n",
    "\n",
    "    def input_validator(self, **kwargs) -> str:\n",
    "        \"\"\"Fill in the input template with kwargs if provided, otherwise return kwargs as str.\"\"\"\n",
    "\n",
    "        if self.input_template:\n",
    "            return self.input_template.format(**kwargs)\n",
    "\n",
    "        return str(kwargs)\n",
    "\n",
    "class GabyBasement(ABC):\n",
    "    \"\"\" Prompt Base Constructor. \"\"\"\n",
    "\n",
    "    _instance = None\n",
    "    _lock = Lock()\n",
    "    client = None\n",
    "    base_model_id = os.getenv(\"BASE_GUFF_LLM_MODEL\", \"hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q3_K_XL\")\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if not hasattr(self, 'client'):\n",
    "            raise AttributeError(\"Ollama client is not initialized. Revise subclass / Base class structure design.\")\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        # Double-checked locking for thread safety\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super().__new__(cls, *args, **kwargs)\n",
    "                try:\n",
    "                    cls._instance.client = ollama.Client(os.getenv(\"OLLAMA_HOST_URL\", \"http://localhost:11434\"))\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Failed to init GabyBasement client: {e}\")\n",
    "\n",
    "            if len(cls._instance.client.list().models) == 0 or cls.base_model_id not in cls._instance.client.list().models  :\n",
    "                cls._instance.client.pull(model=cls.base_model_id)\n",
    "                print(f\"Pulled model: {cls.base_model_id}\")\n",
    "\n",
    "        return cls._instance\n",
    "\n",
    "    def __init_subclass__(cls, prompt: Instructor, **kwargs):\n",
    "        super().__init_subclass__(**kwargs)\n",
    "        cls.prompt = getattr(cls, \"prompt\", prompt)  # donâ€™t overwrite if re-init\n",
    "        cls.name = cls.__qualname__\n",
    "\n",
    "    @property\n",
    "    def system_prompt(self):# -> list[dict[str, Any]]:\n",
    "        text = self.prompt.prompt if isinstance(self.prompt, Instructor) else str(self.prompt)\n",
    "        return [{\"role\": \"system\", \"content\": text}]\n",
    "\n",
    "    def post_process(self, response) -> str:\n",
    "        \"\"\" Post-processes the response from the LLM before returning to the user. Subclass can override this method to implement custom post-processing logic. \"\"\"\n",
    "\n",
    "        return response.message.get('content', None).strip()\n",
    "\n",
    "    def pre_process(self, **kwargs) -> dict:\n",
    "        \"\"\" Pre-processes the input arguments before sending to the LLM. Subclass can override this method to implement custom pre-processing logic. \"\"\"\n",
    "\n",
    "        return kwargs\n",
    "\n",
    "    def run(self, **kwargs) -> str:\n",
    "        \"\"\" Main method to execute the thought chain. \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"client\") or self.client is None:\n",
    "            raise RuntimeError(\"Ollama client is not initialized.\"\n",
    "                               \" Ensure Ollama is running and OLLAMA_HOST_URL is correct.\")\n",
    "\n",
    "        print(f\"Running thought Chain: {self.name}\")\n",
    "\n",
    "        kwargs = self.pre_process(**kwargs)\n",
    "        user_inputs = self.prompt.input_validator(**kwargs)\n",
    "\n",
    "        response = self.client.chat(\n",
    "            model=self.base_model_id,\n",
    "            messages=self.system_prompt + [{\"role\": \"user\", \"content\": user_inputs}],\n",
    "            stream=False,\n",
    "            options={\n",
    "                \"max_tokens\": 100,\n",
    "                \"num_ctx\": 100\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return self.post_process(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ae4fc",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_3_'></a>[Helper Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97fbe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper / Utilities functions\n",
    "\n",
    "import functools\n",
    "\n",
    "\n",
    "def pandas_gatekeeper(func):\n",
    "    \"\"\"\n",
    "    A decorator that executes a SQL query generated by a function and returns a DataFrame.\n",
    "\n",
    "    This decorator intercepts the SQL string returned by the wrapped function,\n",
    "    executes it using the provided BigQuery client, and returns the result\n",
    "    as a pandas DataFrame.\n",
    "\n",
    "    The decorated function must be called with a 'client' keyword argument\n",
    "    of type `google.cloud.bigquery.Client`.\n",
    "\n",
    "    Args:\n",
    "        func: The function to be decorated, which should return a SQL query string.\n",
    "\n",
    "    Returns:\n",
    "        A wrapper function that executes the query and returns a DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If the 'client' keyword argument is not provided or is not a\n",
    "                   `bigquery.Client` instance.\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "\n",
    "        # client = bigquery.Client() -- Since running in notebook.\n",
    "\n",
    "        # Call the original function to get the SQL query string\n",
    "        sql_query = func(*args, **kwargs)\n",
    "\n",
    "        print(f\"--- Executing SQL from '{func.__name__}' ---\")\n",
    "        print(sql_query)\n",
    "\n",
    "        # Execute the query and return the result as a pandas DataFrame\n",
    "        job = client.query(sql_query)\n",
    "        return job.to_dataframe()\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "def summarize_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        total_count = len(df)\n",
    "        missing_count = df[col].isna().sum()\n",
    "        data_type = df[col].dtype\n",
    "\n",
    "        # Check if continuous: numeric with many unique values\n",
    "        if pd.api.types.is_numeric_dtype(df[col]) and df[col].nunique() > 20:\n",
    "            unique_vals = \"continuous\"\n",
    "        else:\n",
    "            unique_vals = df[col].nunique()\n",
    "\n",
    "        yield {\n",
    "            \"data_field_name\": col,\n",
    "            \"missing_count\": missing_count,\n",
    "            \"total_count\": total_count,\n",
    "            \"data_type\": str(data_type),\n",
    "            \"unique_values\": unique_vals\n",
    "        }\n",
    "\n",
    "def upload_dataframe_to_bq(df: pd.DataFrame, project_id: str, dataset_id: str, table_id: str):\n",
    "    \"\"\" Uploads a pandas DataFrame to a specified BigQuery table. \"\"\"\n",
    "\n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        autodetect=True\n",
    "    )\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "    job.result()  # Wait for the job to complete.\n",
    "    print(f\"âœ… Data uploaded to {table_ref}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396dded",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_1_4_'></a>[Data Schemas](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1cd490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reports & Profilers for updating databases in BigQuery between sessions etc.\n",
    "\n",
    "# PROFILERS\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class EpisodeConfig:\n",
    "    input_id: str = SAMPLE_FULL_ID\n",
    "    summary_id: str = SAMPLE_SUMMARY_FULL_ID\n",
    "    # BIGQUERY MODEL STORED LOCS\n",
    "    bq_model_connection: str = BQ_MODEL_CONNECTION\n",
    "    bq_model_endpoint: str = BQ_MODEL_ENDPOINT\n",
    "    bq_model_id: str = BQ_MODEL_ID\n",
    "    default_model_type: str = DEFAULT_MODEL_TYPE\n",
    "    # DEFAULT NAMES\n",
    "    default_project_id: str = BQ_PROJECT_ID\n",
    "    default_dataset_id: str = BQ_DATASET_ID\n",
    "    default_table_id: str = BQ_TABLE_ID\n",
    "\n",
    "@dataclass\n",
    "class EntryReport:\n",
    "    description: str | None = None\n",
    "    data_field_summary: pd.DataFrame | None = None\n",
    "    data_field_description: pd.DataFrame | None = None\n",
    "    numeric_table: pd.DataFrame | None = None\n",
    "\n",
    "@dataclass\n",
    "class MissingDataReport:\n",
    "    missing_pattern: str\n",
    "    missing_count: float\n",
    "    missing_perc: float\n",
    "    data_field_type: str\n",
    "\n",
    "@dataclass\n",
    "class DataProfiler:\n",
    "    # User Inputs\n",
    "    data: pd.DataFrame\n",
    "    user_input_tags: str | list | None = None\n",
    "\n",
    "    # Defining the dataset\n",
    "    description: str | None = None # Describing the dataset in natural language\n",
    "    data_field_summary: pd.DataFrame | None = None # Data field summary table\n",
    "    data_field_description: pd.DataFrame | None = None # Data field summary table with description columns\n",
    "    numeric_table: pd.DataFrame | None = None # Data field summary table with description columns\n",
    "\n",
    "    _send_to_gatekeeper: bool = False\n",
    "    # Episode ID & Configuration\n",
    "    episode_id: str = uuid4().hex\n",
    "    timestamp: str = datetime.now().isoformat()\n",
    "    config: EpisodeConfig | None = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.define_dataset(self._send_to_gatekeeper)\n",
    "        # the EpisodeConfig stored dataset id for the input dataset is set to self.episode_id-self.tiemstamp in prod\n",
    "        # self.config = EpisodeConfig(\n",
    "        #    input_dataset_id=f\"{self.episode_id}-{self.timestamp}\",\n",
    "        #    summary_table_id=f\"{self.episode_id}-{BQ_TABLE_SUMMARY_ID}\"\n",
    "        #)\n",
    "        self.config = EpisodeConfig()\n",
    "\n",
    "    def define_dataset(self, upload_summary: bool = False):\n",
    "        if self.data.shape[0] == 0 or self.user_input_tags is None:\n",
    "            raise ValueError(\"The provided DataFrame is empty or data origin is not specified. Both these are required to start the workflow.\")\n",
    "\n",
    "        # assuming have loaded the model and returned it\n",
    "        self.data_field_summary = pd.DataFrame.from_records(list(summarize_dataframe(self.data)))\n",
    "\n",
    "        print(f\"âœ… Dataset defined with {self.data.shape[0]} rows and {self.data.shape[1]} columns.\")\n",
    "\n",
    "        if upload_summary is True:\n",
    "            upload_dataframe_to_bq(self.data, BQ_PROJECT_ID, BQ_DATASET_ID, BQ_TABLE_ID)\n",
    "            upload_dataframe_to_bq(self.data_field_summary, BQ_PROJECT_ID, BQ_DATASET_ID, BQ_TABLE_SUMMARY_ID)\n",
    "\n",
    "        print(f\"Completed profiling for dataset id: {self.episode_id} and uploaded to BQ.\")\n",
    "\n",
    "    @property\n",
    "    def end_cleaning_report(self) -> EntryReport:\n",
    "        return EntryReport(\n",
    "            description=self.description,\n",
    "            data_field_summary=self.data_field_summary,\n",
    "            data_field_description=self.data_field_description\n",
    "        )\n",
    "\n",
    "    def episode_recap(self):\n",
    "        context_prompt = f\"--- Data Summary ---\"\n",
    "\n",
    "        end_cleaning_report = self.end_cleaning_report\n",
    "\n",
    "        context_prompt += f\"\\nDataset Description: {end_cleaning_report.description}\\n\" if end_cleaning_report.description is not None else \"\\nNo dataset description available.\\n\"\n",
    "        context_prompt += f\"\\nData Field Summary:\\n{end_cleaning_report.data_field_summary.to_markdown(index=False)}\\n\" if end_cleaning_report.data_field_summary is not None else \"\"\n",
    "        context_prompt += f\"\\nData Field Description:\\n{end_cleaning_report.data_field_description.to_markdown(index=False)}\\n\" if end_cleaning_report.data_field_description is not None else \"\"\n",
    "\n",
    "        return context_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets into workspace\n",
    "data = pd.read_csv(LOCAL_SAMPLE_PATH, header=0)\n",
    "gb = DataProfiler(data=data, user_input_tags=[\"sales\", \"beverages\", \"cafe\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b9a742",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[Stage I: Data Documentation and Reporting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884c650",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_2_1_'></a>[Self-Orchestrating Prompt Chain with Ollama](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162cf538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" clean_stage_a.py \"\"\"\n",
    "\n",
    "class DatasetSummarizer(\n",
    "    GabyBasement,\n",
    "    prompt = Instructor(\n",
    "        prompt=\"\"\"\n",
    "        You are a senior data analyst. Based on the datasetâ€™s fields and descriptive metadata, provide a concise summary (no more than 2 sentences) that highlights:\n",
    "        â€¢\tthe datasetâ€™s key characteristics and notable features or patterns,\n",
    "        â€¢\tpotential modeling or analytical objectives it may support, and\n",
    "        â€¢\twhether the dataset contains any unique identifiers.\n",
    "        \"\"\",\n",
    "        input_template=\"\"\"\n",
    "        Dataset descriptive labels: {user_inputs},\n",
    "        Dataset Subset:\n",
    "        {data_table}\n",
    "        \"\"\"\n",
    "    )\n",
    "):\n",
    "    pass\n",
    "\n",
    "class DataFieldMetaDescription(\n",
    "    GabyBasement,\n",
    "    prompt = Instructor(\n",
    "        prompt=\"You are a data analyst. Given the dataset description and a specific data field label, return a concise description of what the data field possibly means in the context of the dataset. Return your response in at most 1 sentence.\",\n",
    "        input_template=\"\"\"\n",
    "        Dataset Description: {data_description}\n",
    "        Data Field Label: {data_label}\n",
    "        Data Sample: {data_sample}\n",
    "        \"\"\"\n",
    "    )\n",
    "):\n",
    "    def run_loop(self, data: pd.DataFrame, data_description: str) -> dict:\n",
    "        \"\"\" Run the description for each data field in the dataframe. \"\"\"\n",
    "\n",
    "        descriptions = {}\n",
    "\n",
    "        for column in data.columns:\n",
    "            sample = data[column].dropna().unique()[:3].tolist()\n",
    "            sample_str = \", \".join(map(str, sample))\n",
    "\n",
    "            description = self.run(\n",
    "                data_description=data_description,\n",
    "                data_label=column,\n",
    "                data_sample=sample_str\n",
    "            )\n",
    "\n",
    "            descriptions[column] = [\n",
    "                {\n",
    "                    'name': self.post_process(description),\n",
    "                    'data_type': str(data[column].dtype),\n",
    "                    'description': description\n",
    "                }\n",
    "            ]\n",
    "\n",
    "        return descriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899eebae",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_2_2_'></a>[BigQuery Gatekeeper](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_DESCRIBE_DATA_FIELD_LABEL= \"\"\"\n",
    "SELECT\n",
    "  data_field_name,\n",
    "  AI.GENERATE( ('The data field name ',\n",
    "      data_field_name,\n",
    "      'with values of data type,',\n",
    "      data_type,\n",
    "      'is one of the dataset column labels of a dataset with description: cafe sale logs. In a sentence, define what each data field represent.'),\n",
    "    connection_id => '{connection_id}',\n",
    "    endpoint => '{endpoint}',\n",
    "    output_schema => 'data_field_name STRING, description STRING').description\n",
    "FROM\n",
    "  {data_summary_id};\n",
    "\"\"\"\n",
    "\n",
    "# 2 methods in this file to distinguish numeric values.\n",
    "# Ordinal: categories have a natural order but no consistent distance between them (e.g., low, medium, high)\n",
    "SQL_DETECT_NUMERIC_FIELD = \"\"\"\n",
    "SELECT\n",
    "  data_field_name,\n",
    "  AI.GENERATE( ('The data field name ',\n",
    "      data_field_name,\n",
    "      'with values of data type,',\n",
    "      data_type,\n",
    "      'is one of the dataset column labels of a dataset with description: cafe sale logs.'\n",
    "      'Classify the dataset field into one of: Nominal, Ordinal, Continuous, Unknown, if the data type is numerical and if not, return Unknown. Return the result strictly as: \"<Nominal|Ordinal|Continuous|Unknown>\"'\n",
    "      ),\n",
    "    connection_id => '{connection_id}',\n",
    "    endpoint => '{endpoint}',\n",
    "    output_schema => 'data_field_name STRING, numeric_type STRING').numeric_type\n",
    "FROM\n",
    "  {data_summary_id};\n",
    "\"\"\"\n",
    "\n",
    "@pandas_gatekeeper\n",
    "def describe_data_field(\n",
    "    data_summary_id: str,\n",
    "    connection_id: str = BQ_MODEL_CONNECTION,\n",
    "    endpoint: str = DEFAULT_MODEL_TYPE\n",
    "):\n",
    "    return SQL_DESCRIBE_DATA_FIELD_LABEL.format(\n",
    "        data_summary_id=data_summary_id,\n",
    "        connection_id=connection_id,\n",
    "        endpoint=endpoint\n",
    "    )\n",
    "\n",
    "@pandas_gatekeeper\n",
    "def detect_numeric_field(\n",
    "    data_summary_id: str,\n",
    "    connection_id: str = BQ_MODEL_CONNECTION,\n",
    "    endpoint: str = DEFAULT_MODEL_TYPE\n",
    "):\n",
    "    return SQL_DETECT_NUMERIC_FIELD.format(\n",
    "        data_summary_id=data_summary_id,\n",
    "        connection_id=connection_id,\n",
    "        endpoint=endpoint\n",
    "    )\n",
    "\n",
    "#gb.data_field_description = describe_data_field(\n",
    "#    data_summary_id=SAMPLE_SUMMARY_FULL_ID,\n",
    "#    connection_id=BQ_MODEL_CONNECTION,\n",
    "#    endpoint=DEFAULT_MODEL_TYPE\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af2e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Executing SQL from 'detect_numeric_field' ---\n",
      "\n",
      "SELECT\n",
      "  data_field_name,\n",
      "  AI.GENERATE( ('The data field name ',\n",
      "      data_field_name,\n",
      "      'with values of data type,',\n",
      "      data_type,\n",
      "      'is one of the dataset column labels of a dataset with description: cafe sale logs.'\n",
      "      'Classify the dataset field into one of: Nominal, Ordinal, Continuous, Unknown, if the data type is numerical and if not, return Unknown. Return the result strictly as: \"<Nominal|Ordinal|Continuous|Unknown>\"'\n",
      "      ),\n",
      "    connection_id => 'projects/481034637222/locations/australia-southeast1/connections/__default_cloudresource_connection__',\n",
      "    endpoint => 'gemini-2.5-flash',\n",
      "    output_schema => 'data_field_name STRING, numeric_type STRING').numeric_type\n",
      "FROM\n",
      "  genial-motif-472804-s1.cleaning_service.field_summary;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mimiphan/mimeus-app/backend/gaby/.venv/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1994: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_field_name</th>\n",
       "      <th>numeric_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Payment Method</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Location</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Total Spent</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantity</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transaction ID</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Price Per Unit</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transaction Date</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_field_name numeric_type\n",
       "0    Payment Method      Unknown\n",
       "1              Item      Unknown\n",
       "2          Location      Unknown\n",
       "3       Total Spent      Unknown\n",
       "4          Quantity      Unknown\n",
       "5    Transaction ID      Unknown\n",
       "6    Price Per Unit      Unknown\n",
       "7  Transaction Date      Unknown"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_numeric_field(data_summary_id=SAMPLE_SUMMARY_FULL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac32c0",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_2_3_'></a>[To Run Workflow](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning_pipeline(report: DataProfiler) -> DataProfiler:\n",
    "    \"\"\" Main function to run the data cleaning pipeline. \"\"\"\n",
    "\n",
    "    report.description = DatasetSummarizer().run(\n",
    "        user_inputs=report.user_input,\n",
    "        data_table=report.data.head(3).to_string(index=False)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        report.data_field_description = describe_data_field(\n",
    "            connection_id=report.config.bq_model_connection,\n",
    "            model_endpoint=report.config.default_model_type,\n",
    "        )\n",
    "\n",
    "        report.numeric_table = detect_numeric_field(\n",
    "            connection_id=report.config.bq_model_connection,\n",
    "            endpoint=report.config.default_model_type,\n",
    "            data_summary_id=report.config.summary_id\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error using GCP model, falling back to local model (takes longer to run):\", e)\n",
    "\n",
    "        report.data_field_description = DataFieldMetaDescription().run_loop(\n",
    "            data=report.data,\n",
    "            data_description=report.description\n",
    "        )\n",
    "        report.numeric_table = None\n",
    "\n",
    "    return report\n",
    "\n",
    "gb = data_cleaning_pipeline(gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063aa0ff",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_2_4_'></a>[Final Report](#toc0_)\n",
    "\n",
    "A data report is returned after each stage to carry onto the next stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d15f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_field_name</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>data_type</th>\n",
       "      <th>unique_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transaction ID</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item</td>\n",
       "      <td>333</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quantity</td>\n",
       "      <td>138</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Price Per Unit</td>\n",
       "      <td>179</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Total Spent</td>\n",
       "      <td>173</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Payment Method</td>\n",
       "      <td>2579</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Location</td>\n",
       "      <td>3265</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transaction Date</td>\n",
       "      <td>159</td>\n",
       "      <td>10000</td>\n",
       "      <td>object</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_field_name  missing_count  total_count data_type  unique_values\n",
       "0    Transaction ID              0        10000    object          10000\n",
       "1              Item            333        10000    object             10\n",
       "2          Quantity            138        10000    object              7\n",
       "3    Price Per Unit            179        10000    object              8\n",
       "4       Total Spent            173        10000    object             19\n",
       "5    Payment Method           2579        10000    object              5\n",
       "6          Location           3265        10000    object              4\n",
       "7  Transaction Date            159        10000    object            367"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.data_field_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be81c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the dataset in 2 sentences:\n",
      "\n",
      "This dataset appears to track sales transactions from a cafe, capturing details such as items purchased, quantities, prices per unit, and total spent at each location. The data contains some inconsistencies due to missing values or unknown payment methods, but overall suggests that the cafe sells beverages like coffee and cake, with customers often purchasing in-store.\n"
     ]
    }
   ],
   "source": [
    "print(gb.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935b8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_field_name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Spent</td>\n",
       "      <td>This field represents the total amount of mone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Payment Method</td>\n",
       "      <td>This field represents the method of payment us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Item</td>\n",
       "      <td>The 'Item' field represents the name of the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Location</td>\n",
       "      <td>This field represents the geographical locatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantity</td>\n",
       "      <td>Represents the number of items sold in a cafe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transaction ID</td>\n",
       "      <td>cafe sale logs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Price Per Unit</td>\n",
       "      <td>This field represents the cost of a single ite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transaction Date</td>\n",
       "      <td>This data field represents the date of a trans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    data_field_name                                        description\n",
       "0       Total Spent  This field represents the total amount of mone...\n",
       "1    Payment Method  This field represents the method of payment us...\n",
       "2              Item  The 'Item' field represents the name of the pr...\n",
       "3          Location  This field represents the geographical locatio...\n",
       "4          Quantity  Represents the number of items sold in a cafe ...\n",
       "5    Transaction ID                                     cafe sale logs\n",
       "6    Price Per Unit  This field represents the cost of a single ite...\n",
       "7  Transaction Date  This data field represents the date of a trans..."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.data_field_description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3982947",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_3_'></a>[Stage II: Missing Data Values](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767753b",
   "metadata": {},
   "source": [
    "#### <a id='toc1_1_3_1_'></a>[Helper Functions & Utils](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28797e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_MISSING_DATA_PATTERN_METHOD = \"\"\"\n",
    "SELECT\n",
    "  data_field_name,\n",
    "  data_type,\n",
    "  AI.GENERATE( ('The data field name ',\n",
    "      data_field_name,\n",
    "      'is sourced from dataset with description: car sale logs.',\n",
    "\n",
    "      'Data field is of data type,',\n",
    "      data_type,\n",
    "      'and has a total',\n",
    "      missing_count,\n",
    "      'number of missing values. Suggest the most common method in dealing with missing datasets of this data type and context. Your response must only contain one of the values fromthe following: [\\'imputation\\', \\'drop_missing\\', ].'),\n",
    "    connection_id => '{connection_id}',\n",
    "    endpoint => '{endpoint}',\n",
    "    output_schema => 'data_field_name STRING, description STRING').description\n",
    "FROM\n",
    "  {data_summary_id};\n",
    "\"\"\"\n",
    "\n",
    "@pandas_gatekeeper\n",
    "def suggest_missing_data_pattern_method(\n",
    "    data_summary_id: str,\n",
    "    connection_id: str = BQ_MODEL_CONNECTION,\n",
    "    endpoint: str = DEFAULT_MODEL_TYPE\n",
    "):\n",
    "    return SQL_MISSING_DATA_PATTERN_METHOD.format(\n",
    "        data_summary_id=data_summary_id,\n",
    "        connection_id=connection_id,\n",
    "        endpoint=endpoint\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9a135",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_4_'></a>[Stage III: Data Anomality Detection For Continous Dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_fields(df: pd.DataFrame,\n",
    "                          numeric_types=(\"INT64\", \"FLOAT64\", \"NUMERIC\"),\n",
    "                          min_unique=20,\n",
    "                          min_unique_ratio=0.05):\n",
    "    \"\"\"\n",
    "    Return only the continuous fields from a dataset summary DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Must have columns [data_field_name, data_type, unique_values, total_count].\n",
    "        numeric_types (tuple): BigQuery numeric types considered numeric.\n",
    "        min_unique (int): Minimum unique values threshold.\n",
    "        min_unique_ratio (float): Minimum ratio (unique / total_count).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing only continuous fields.\n",
    "    \"\"\"\n",
    "    mask = (\n",
    "        df[\"data_type\"].isin(numeric_types)\n",
    "        & (\n",
    "            (df[\"unique_values\"] > min_unique) |\n",
    "            (df[\"unique_values\"] / df[\"total_count\"] > min_unique_ratio)\n",
    "        )\n",
    "        & (df[\"unique_values\"] < df[\"total_count\"])  # exclude pure IDs\n",
    "    )\n",
    "    return df.loc[mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357fd6e",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Agent Reinforcement Learning & LifeCycle](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b953b737",
   "metadata": {},
   "source": [
    "Post user sessions Workflow:\n",
    "\n",
    "- Subset or sample of the dataset is stored in vector form to enable faster key-value storage in real-time. This is beneficial in the case when the receiving datasets are large and prolonging the time in recalling the agent's lookup table of previous observations, states and rewards. \n",
    "\n",
    "Reward:\n",
    "- Given the model objective, the ratio of invested time to succeess is used to scale the estimating coefficents of the predicting outcomes of the agent's decision making process.\n",
    "\n",
    "How BigQuery comes into play\n",
    "\n",
    "- As Gaby's Gatekeeper, BigQuery integrates the update and storage fucntion with `AI.FORECAST` . This allows Gaby to forecast outcomes of future decisions with BigQuery ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243045b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATA_OBSERVATION_ID = \"observations\"\n",
    "BQ_DATA_INSIGHT_ID = \"insights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "N_STEPS = 10 # Number of steps\n",
    "\n",
    "def every_n_steps(n: int):\n",
    "    \"\"\"Decorator to ensure a function only runs every n steps.\"\"\"\n",
    "    def decorator(func):\n",
    "        counter = {\"step\": 0}  # mutable closure\n",
    "\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            counter[\"step\"] += 1\n",
    "            if counter[\"step\"] % n == 0:\n",
    "                return func(*args, **kwargs)\n",
    "            else:\n",
    "                # Optional: return None or a placeholder when skipped\n",
    "                return None\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@every_n_steps(N_STEPS)\n",
    "@pandas_gatekeeper\n",
    "def fetch_recent_observations(days: int = 7) -> str:\n",
    "    \"\"\" Fetch recent observations from the last number of days. \"\"\"\n",
    "    return f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{BQ_PROJECT_ID}.{BQ_DATASET_OBSERVATION_ID}.observations`\n",
    "    WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {days} DAY)\n",
    "    ORDER BY timestamp DESC\n",
    "    \"\"\"\n",
    "\n",
    "@every_n_steps(N_STEPS)\n",
    "@pandas_gatekeeper\n",
    "def store_agent_insight(action: str,\n",
    "                        reward: float,\n",
    "                        metadata: dict) -> str:\n",
    "    \"\"\" Log an agent's decision, reward, and metadata into BigQuery. \"\"\"\n",
    "    return f\"\"\"\n",
    "    INSERT INTO `{BQ_PROJECT_ID}.{BQ_DATA_INSIGHT_ID}.rewards`\n",
    "    (timestamp, action, reward, metadata)\n",
    "    VALUES (\n",
    "        CURRENT_TIMESTAMP(),\n",
    "        '{action}',\n",
    "        {reward},\n",
    "        TO_JSON_STRING({metadata})\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "@every_n_steps(N_STEPS)\n",
    "@pandas_gatekeeper\n",
    "def forecast_agent_rewards(horizon: int = 10) -> str:\n",
    "    \"\"\" Forecast expected rewards for next N steps using BigQuery ML. \"\"\"\n",
    "    return f\"\"\"\n",
    "    SELECT *\n",
    "    FROM ML.FORECAST(\n",
    "      MODEL `{BQ_PROJECT_ID}.{BQ_DATA_INSIGHT_ID}.reward_forecaster`,\n",
    "      STRUCT({horizon} AS horizon, 0.9 AS confidence_level)\n",
    "    )\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
