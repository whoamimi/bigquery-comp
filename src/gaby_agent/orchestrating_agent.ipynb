{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce14926",
   "metadata": {},
   "source": [
    "# Orchestrating Agent \n",
    "\n",
    "Accessing models via:\n",
    "- Ollama servers\n",
    "- Google BigQuery AI / Vertex AI\n",
    "\n",
    "Resources\n",
    "- [BigQuery ML Methods with Gemini](https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb2a9e",
   "metadata": {},
   "source": [
    "**GGUF Instruct Models**\n",
    "\n",
    "GGUF Model types are compressed variants of the original model for inferencing tasks in workspaces with lower compute configs. Default server used is Ollama and to pull and run the model for inferencing is prefixed in cli: `ollama run *`\n",
    "\n",
    "```yaml\n",
    "- model_name: base\n",
    "  model_id:\n",
    "    dev: hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q3_K_L\n",
    "    prod: hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q5_K_S\n",
    "  url: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF\n",
    "- model_name: sql_coder\n",
    "  model_id:\n",
    "    dev: hf.co/TheBloke/sqlcoder-GGUF:Q3_K_S\n",
    "    prod: hf.co/TheBloke/sqlcoder-GGUF:Q4_K_S\n",
    "  url: https://huggingface.co/TheBloke/sqlcoder-GGUF\n",
    "- model_name: python_coder\n",
    "  model_id:\n",
    "    dev: hf.co/TheBloke/CodeLlama-7B-Python-GGUF:Q3_K_M\n",
    "    prod: hf.co/TheBloke/CodeLlama-7B-Python-GGUF:Q4_K_M\n",
    "  url: https://huggingface.co/TheBloke/CodeLlama-7B-Python-GGUF\n",
    "```\n",
    "Ones in Testing:\n",
    "- Chat GPT: https://huggingface.co/unsloth/gpt-oss-20b-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b48278ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path already set to default root directory: /Users/mimiphan/mimeus-app/backend/gaby\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_dev_workspace(root_folder_name: str = 'gaby'):\n",
    "    \"\"\" Call in files / notebooks if running workspace in sub-directory path. \"\"\"\n",
    "\n",
    "    if Path.cwd().stem == root_folder_name:\n",
    "        print(f'Path already set to default root directory: {Path.cwd()}')\n",
    "        return\n",
    "    else:\n",
    "        print('Initialized workspace currently at directory:', Path.cwd())\n",
    "\n",
    "    current = Path().resolve()\n",
    "    for parent in [current, *current.parents]:\n",
    "        if parent.name == root_folder_name:\n",
    "            os.chdir(parent)  # change working directory\n",
    "            print(f\"ðŸ“‚ Working directory set to: {parent}\")\n",
    "            returnÂ \n",
    "\n",
    "    raise FileNotFoundError(f\"Root folder '{root_folder_name}' not found.\")\n",
    "\n",
    "setup_dev_workspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d75d716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://11434-01jxw838wanez4bbetq84ep1qv.cloudspaces.litng.ai/'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('/Users/mimiphan/mimeus-app/backend/gaby/.env.local')\n",
    "\n",
    "LIGHTNING_OLLAMA_HOST_URL = os.getenv('LIGHTNING_OLLAMA_HOST_URL', '')\n",
    "MODEL_STACK = [\n",
    "    \"hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q5_K_S\", # BASE INSTRUCT HELPER TOOL\n",
    "    \"hf.co/TheBloke/sqlcoder-GGUF:Q4_K_M\", # SQL CODER\n",
    "    \"hf.co/TheBloke/CodeLlama-7B-Python-GGUF:Q3_K_M\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301f409",
   "metadata": {},
   "source": [
    "## Ollama Client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d583c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ollama\n",
    "import ollama\n",
    "import json, time, hashlib, threading\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "PromptLike = Union[str, Dict[str, Any], List[Dict[str, str]]]\n",
    "\n",
    "class OllamaCall:\n",
    "    \"\"\"\n",
    "    Decorator class to wrap a function that returns a prompt/messages payload.\n",
    "    - Your function should return either:\n",
    "        1) str -> treated as `prompt` (for /api/generate)\n",
    "        2) list[{\"role\",\"content\"}] -> treated as chat messages (for /api/chat)\n",
    "        3) dict -> passed through; must include 'prompt' (generate) or 'messages' (chat)\n",
    "\n",
    "    Features:\n",
    "    - mode='chat' or 'generate'\n",
    "    - streaming optional (stores concatenated text)\n",
    "    - LRU cache with JSONL persistence\n",
    "    - thread-safe\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        mode: str = \"chat\",            # \"chat\" | \"generate\"\n",
    "        stream: bool = False,\n",
    "        max_history: int = 1000,\n",
    "        save_path: Optional[Union[str, Path]] = None,\n",
    "        key_fn: Optional[Callable[[Dict[str, Any]], str]] = None,\n",
    "        extra_params: Optional[Dict[str, Any]] = None,  # e.g., {\"temperature\": 0.2}\n",
    "    ):\n",
    "        assert mode in {\"chat\", \"generate\"}\n",
    "        self.model = model\n",
    "        self.mode = mode\n",
    "        self.stream = stream\n",
    "        self.max_history = max_history\n",
    "        self.save_path = Path(save_path) if save_path else None\n",
    "        self.key_fn = key_fn\n",
    "        self.extra_params = extra_params or {}\n",
    "\n",
    "        self._lock = threading.Lock()\n",
    "        self._lru: \"OrderedDict[str, Dict[str, Any]]\" = OrderedDict()\n",
    "\n",
    "        # Warm-load existing JSONL (optional)\n",
    "        if self.save_path and self.save_path.exists():\n",
    "            try:\n",
    "                with self.save_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        rec = json.loads(line)\n",
    "                        k = rec.get(\"key\")\n",
    "                        if k:\n",
    "                            self._lru[k] = rec\n",
    "                # keep only most recent max_history\n",
    "                while len(self._lru) > self.max_history:\n",
    "                    self._lru.popitem(last=False)\n",
    "            except Exception:\n",
    "                # don't blow up on partial/corrupt logs\n",
    "                pass\n",
    "\n",
    "    def _norm_payload(self, payload: PromptLike) -> Dict[str, Any]:\n",
    "        \"\"\"Normalize user return into {'messages': ...} or {'prompt': ...}.\"\"\"\n",
    "        if isinstance(payload, str):\n",
    "            return {\"prompt\": payload}\n",
    "        if isinstance(payload, list):\n",
    "            # assume chat messages schema\n",
    "            return {\"messages\": payload}\n",
    "        if isinstance(payload, dict):\n",
    "            if \"prompt\" in payload or \"messages\" in payload:\n",
    "                return payload\n",
    "        raise TypeError(\"Return a str, a messages list, or a dict with 'prompt' or 'messages'.\")\n",
    "\n",
    "    def _default_key(self, request: Dict[str, Any]) -> str:\n",
    "        # Stable key from: model + mode + request + extra_params\n",
    "        blob = json.dumps(\n",
    "            {\"model\": self.model, \"mode\": self.mode, \"req\": request, \"extra\": self.extra_params},\n",
    "            sort_keys=True,\n",
    "            ensure_ascii=False,\n",
    "        )\n",
    "        return hashlib.sha256(blob.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def _record(self, key: str, request: Dict[str, Any], response_text: str) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"key\": key,\n",
    "            \"ts\": time.time(),\n",
    "            \"model\": self.model,\n",
    "            \"mode\": self.mode,\n",
    "            \"request\": request,\n",
    "            \"response\": response_text,\n",
    "            \"params\": self.extra_params,\n",
    "        }\n",
    "\n",
    "    def _persist(self, rec: Dict[str, Any]):\n",
    "        if not self.save_path:\n",
    "            return\n",
    "        self.save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with self.save_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    def history(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Return recent conversation records (most-recent last).\"\"\"\n",
    "        with self._lock:\n",
    "            return list(self._lru.values())\n",
    "\n",
    "    def get(self, key: str) -> Optional[Dict[str, Any]]:\n",
    "        with self._lock:\n",
    "            return self._lru.get(key)\n",
    "\n",
    "    def __call__(self, fn: Callable[..., PromptLike]):\n",
    "        def wrapper(*args, **kwargs) -> str:\n",
    "            payload = self._norm_payload(fn(*args, **kwargs))\n",
    "\n",
    "            # Build request body\n",
    "            if self.mode == \"chat\":\n",
    "                if \"messages\" not in payload:\n",
    "                    # if only 'prompt' given, wrap as one user message\n",
    "                    payload = {\"messages\": [{\"role\": \"user\", \"content\": payload[\"prompt\"]}]}\n",
    "                request = {\"model\": self.model, \"messages\": payload[\"messages\"], **self.extra_params}\n",
    "            else:\n",
    "                if \"prompt\" not in payload:\n",
    "                    # if only 'messages' given, squeeze into a single prompt\n",
    "                    merged = \"\\n\".join([m.get(\"content\", \"\") for m in payload[\"messages\"]])\n",
    "                    payload = {\"prompt\": merged}\n",
    "                request = {\"model\": self.model, \"prompt\": payload[\"prompt\"], **self.extra_params}\n",
    "\n",
    "            # Cache key\n",
    "            key = self.key_fn(request) if self.key_fn else self._default_key(request)\n",
    "\n",
    "            # LRU check\n",
    "            with self._lock:\n",
    "                if key in self._lru:\n",
    "                    rec = self._lru.pop(key)         # mark as recently used\n",
    "                    self._lru[key] = rec\n",
    "                    return rec[\"response\"]\n",
    "\n",
    "            # Call Ollama\n",
    "            if self.mode == \"chat\":\n",
    "                if self.stream:\n",
    "                    text = []\n",
    "                    for chunk in ollama.chat(model=self.model, messages=request[\"messages\"], stream=True, **self.extra_params):\n",
    "                        part = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "                        if part:\n",
    "                            text.append(part)\n",
    "                    out = \"\".join(text)\n",
    "                else:\n",
    "                    resp = ollama.chat(model=self.model, messages=request[\"messages\"], **self.extra_params)\n",
    "                    out = resp[\"message\"][\"content\"]\n",
    "            else:\n",
    "                if self.stream:\n",
    "                    text = []\n",
    "                    for chunk in ollama.generate(model=self.model, prompt=request[\"prompt\"], stream=True, **self.extra_params):\n",
    "                        part = chunk.get(\"response\", \"\")\n",
    "                        if part:\n",
    "                            text.append(part)\n",
    "                    out = \"\".join(text)\n",
    "                else:\n",
    "                    resp = ollama.generate(model=self.model, prompt=request[\"prompt\"], **self.extra_params)\n",
    "                    out = resp[\"response\"]\n",
    "\n",
    "            rec = self._record(key, request, out)\n",
    "\n",
    "            # Update LRU + persist\n",
    "            with self._lock:\n",
    "                self._lru[key] = rec\n",
    "                # enforce LRU size\n",
    "                while len(self._lru) > self.max_history:\n",
    "                    self._lru.popitem(last=False)\n",
    "            self._persist(rec)\n",
    "\n",
    "            return out\n",
    "\n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fe850da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "client = ollama.Client(LIGHTNING_OLLAMA_HOST_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56711131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(model='hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q5_K_S', modified_at=datetime.datetime(2025, 9, 27, 7, 43, 31, 225350, tzinfo=TzInfo(UTC)), digest='2ea8ba65d59e6cfee05595e37890bc6a68d91aac807d9d12474b74cddb48128c', size=2269514322, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.21B', quantization_level='unknown'))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list().models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cf122",
   "metadata": {},
   "source": [
    "### Detailed Config of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "76fe8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import ShowResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cce5b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "m: ShowResponse = client.show(model=\"hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q5_K_S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee830c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama Client Error model 'hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q3_K_L' not found (status code: 404)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    m: ShowResponse = client.show(model=\"hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q3_K_L\")\n",
    "except ollama.ResponseError as e:\n",
    "    print('Ollama Client Error', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "13b547df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Modelfile generated by \"ollama show\"\\n# To build a new Modelfile based on this, replace FROM with:\\n# FROM hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q5_K_S\\n\\nFROM /root/.ollama/models/blobs/sha256-fc58b1880ce451d10f9aa11dd1b566a50cd41b1f72981c5f8a35313e397358f7\\nTEMPLATE \"\"\"{{- if or .System .Tools }}<|start_header_id|>system<|end_header_id|>\\n{{- if .System }}\\n\\n{{ .System }}\\n{{- end }}\\n{{- if .Tools }}\\n\\nCutting Knowledge Date: December 2023\\n\\nWhen you receive a tool call response, use the output to format an answer to the orginal user question.\\n\\nYou are a helpful assistant with tool calling capabilities.\\n{{- end }}<|eot_id|>\\n{{- end }}\\n{{- range $i, $_ := .Messages }}\\n{{- $last := eq (len (slice $.Messages $i)) 1 }}\\n{{- if eq .Role \"user\" }}<|start_header_id|>user<|end_header_id|>\\n{{- if and $.Tools $last }}\\n\\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\\n\\nRespond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\\n\\n{{ range $.Tools }}\\n{{- . }}\\n{{ end }}\\nQuestion: {{ .Content }}<|eot_id|>\\n{{- else }}\\n\\n{{ .Content }}<|eot_id|>\\n{{- end }}{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ end }}\\n{{- else if eq .Role \"assistant\" }}<|start_header_id|>assistant<|end_header_id|>\\n{{- if .ToolCalls }}\\n{{ range .ToolCalls }}\\n{\"name\": \"{{ .Function.Name }}\", \"parameters\": {{ .Function.Arguments }}}{{ end }}\\n{{- else }}\\n\\n{{ .Content }}\\n{{- end }}{{ if not $last }}<|eot_id|>{{ end }}\\n{{- else if eq .Role \"tool\" }}<|start_header_id|>ipython<|end_header_id|>\\n\\n{{ .Content }}<|eot_id|>{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ end }}\\n{{- end }}\\n{{- end }}\"\"\"\\nPARAMETER stop <|start_header_id|>\\nPARAMETER stop <|end_header_id|>\\nPARAMETER stop <|eot_id|>\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ea63a7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- if or .System .Tools }}<|start_header_id|>system<|end_header_id|>\n",
      "{{- if .System }}\n",
      "\n",
      "{{ .System }}\n",
      "{{- end }}\n",
      "{{- if .Tools }}\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "\n",
      "When you receive a tool call response, use the output to format an answer to the orginal user question.\n",
      "\n",
      "You are a helpful assistant with tool calling capabilities.\n",
      "{{- end }}<|eot_id|>\n",
      "{{- end }}\n",
      "{{- range $i, $_ := .Messages }}\n",
      "{{- $last := eq (len (slice $.Messages $i)) 1 }}\n",
      "{{- if eq .Role \"user\" }}<|start_header_id|>user<|end_header_id|>\n",
      "{{- if and $.Tools $last }}\n",
      "\n",
      "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
      "\n",
      "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\n",
      "\n",
      "{{ range $.Tools }}\n",
      "{{- . }}\n",
      "{{ end }}\n",
      "Question: {{ .Content }}<|eot_id|>\n",
      "{{- else }}\n",
      "\n",
      "{{ .Content }}<|eot_id|>\n",
      "{{- end }}{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{{ end }}\n",
      "{{- else if eq .Role \"assistant\" }}<|start_header_id|>assistant<|end_header_id|>\n",
      "{{- if .ToolCalls }}\n",
      "{{ range .ToolCalls }}\n",
      "{\"name\": \"{{ .Function.Name }}\", \"parameters\": {{ .Function.Arguments }}}{{ end }}\n",
      "{{- else }}\n",
      "\n",
      "{{ .Content }}\n",
      "{{- end }}{{ if not $last }}<|eot_id|>{{ end }}\n",
      "{{- else if eq .Role \"tool\" }}<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "{{ .Content }}<|eot_id|>{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{{ end }}\n",
      "{{- end }}\n",
      "{{- end }}\n"
     ]
    }
   ],
   "source": [
    "print(m.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "794a5a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_computed_fields__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_setattr_handlers__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__replace__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_recursion__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_get_value',\n",
       " '_iter',\n",
       " '_setattr_handler',\n",
       " 'capabilities',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'details',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'get',\n",
       " 'json',\n",
       " 'license',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'modelfile',\n",
       " 'modelinfo',\n",
       " 'modified_at',\n",
       " 'parameters',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'template',\n",
       " 'update_forward_refs',\n",
       " 'validate']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46923a3b",
   "metadata": {},
   "source": [
    "## Missing Dataset Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Options\n",
    "\n",
    "DEFAULT_OPTIONS = Options(\n",
    "    num_ctx=1024,         # shorter context â†’ less overhead\n",
    "    temperature=0.3,      # still stable, but a touch livelier\n",
    "    top_p=0.9,            # good nucleus sampling\n",
    "    top_k=40,             # typical safe default\n",
    "    repeat_penalty=1.05,  # lighter repetition check â†’ less compute\n",
    "    num_predict=128,      # cap on tokens (speeds up response)\n",
    "    num_thread=6,         # match physical CPU cores (adjust to your machine)\n",
    "    num_gpu=1,            # offload to GPU if you have one\n",
    "    low_vram=False,       # only True if youâ€™re memory-starved\n",
    "    f16_kv=True,          # faster key/value cache\n",
    "    use_mmap=True,        # mmap the model for faster loading\n",
    "    use_mlock=False,      # set True if you want to lock into RAM\n",
    "    seed=None             # nondeterministic, so cache doesnâ€™t collide\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2109145",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215265c4",
   "metadata": {},
   "source": [
    "- OrchestrateBook = high-level workflow (like a book outline: Chapters).\n",
    "- OrchestrateChapters = sub-stages within a given Story step (like the Scenes in a Chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47bfaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import ChatResponse\n",
    "from dataclasses import dataclass, field\n",
    "from src.gaby_agent.core.agent._core import Instructor, GabyBasement\n",
    "\n",
    "from src.gaby_agent.core.config import LocalConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "757aa1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LocalConfig()\n",
    "REASONING_AGENT_ID = \"hf.co/ggml-org/gpt-oss-20b-GGUF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5c61ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ollama.Client(config.lightning_ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e0be7b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.pull(REASONING_AGENT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6cdc6039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(model='hf.co/ggml-org/gpt-oss-20b-GGUF:latest', modified_at=datetime.datetime(2025, 9, 27, 17, 9, 25, 749660, tzinfo=TzInfo(UTC)), digest='eb9eabc5cfb912fb3e99d1c3ea9b648fac7d808369e9f70b484ce9054ee2aed9', size=12109567696, details=ModelDetails(parent_model='', format='gguf', family='gpt-oss', families=['gpt-oss'], parameter_size='20.9B', quantization_level='unknown')),\n",
       " Model(model='hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q5_K_S', modified_at=datetime.datetime(2025, 9, 27, 16, 6, 2, 145118, tzinfo=TzInfo(UTC)), digest='2ea8ba65d59e6cfee05595e37890bc6a68d91aac807d9d12474b74cddb48128c', size=2269514322, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='3.21B', quantization_level='unknown'))]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list().models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e2e4e58c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__new__() takes exactly one argument (the type to instantiate)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_history\u001b[38;5;241m.\u001b[39mappend(entry)\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n\u001b[0;32m---> 68\u001b[0m writer \u001b[38;5;241m=\u001b[39m \u001b[43mSummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMissing Data Values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnarrate_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mimeus-app/backend/gaby/src/gaby_agent/core/agent/_core.py:42\u001b[0m, in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     host_url \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIGHTNING_HOST_OLLAMA_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIGHTNING_OLLAMA_HOST_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01melse\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOLLAMA_HOST_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mClient(host_url)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to init GabyBasement client: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: object.__new__() takes exactly one argument (the type to instantiate)"
     ]
    }
   ],
   "source": [
    "# Narrator\n",
    "import inspect\n",
    "\n",
    "K_INTERVAL = 5\n",
    "\n",
    "class Summarizer(\n",
    "    GabyBasement,\n",
    "    prompt=Instructor(\n",
    "        prompt=\"You are a project summarizing assistant that summarizes a data team's progression in data cleaning workflow. Given the user's input project workflow description, history actions and evaluation, your task is to summarize the events in no more than 5 sentences.\",\n",
    "        input_template=\"\"\"\n",
    "        Data Cleaning Project Workflow Steps\n",
    "        {project_meta}\n",
    "\n",
    "        Previous Actions\n",
    "        {action_history}\n",
    "\n",
    "        Current Stage\n",
    "        {most_recent_stage}\n",
    "        \"\"\"\n",
    "    )\n",
    "):\n",
    "    def __init__(self, project_stages: str, narrate_interval: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.k = narrate_interval\n",
    "        self.history = []\n",
    "        self.action_history = []\n",
    "        self.project_meta = project_stages # should be the substages of one of the data processing workflow e.g. data cleaning - missing dataset handler etc.\n",
    "\n",
    "    def post_process(self, response: ChatResponse):\n",
    "        content = f\"{response.created_at}\\n\"\n",
    "        content += response.message.get('content', None).strip()\n",
    "        return content\n",
    "\n",
    "    def step(self):\n",
    "        if len(self.history) % self.k == 0:\n",
    "            print(\"Summarizing previous actions now.\")\n",
    "\n",
    "            kwargs = {}\n",
    "\n",
    "            if len(self.history) == 0:\n",
    "                kwargs.update({\"action_history\": \"\"})\n",
    "                kwargs.update({\"most_recent_stage\": \"\"})\n",
    "            else:\n",
    "                kwargs.update({\"action_history\": \"\".join(self.history)})\n",
    "                kwargs.update({\"most_recent_stage\": self.history[-1]})\n",
    "\n",
    "            kwargs.update({\"project_meta\": self.project_meta})\n",
    "\n",
    "            response = self.run(**kwargs)\n",
    "            self.history += [response]\n",
    "        else:\n",
    "            print(f\"Not summarizing actions, current counter: {len(self.history)}.\")\n",
    "\n",
    "    def add_action(self, cls):\n",
    "        \"\"\"\n",
    "        Method version of the footprint decorator.\n",
    "        Example:\n",
    "            @summary.add_footprint\n",
    "            class MyAgent: ...\n",
    "        \"\"\"\n",
    "        doc = inspect.getdoc(cls) or \"No description available.\"\n",
    "        entry = f\"Class {cls.__name__}: {doc}\\n\"\n",
    "        self.action_history.append(entry)\n",
    "        return cls\n",
    "\n",
    "writer = Summarizer(project_stages=\"Missing Data Values\", narrate_interval=K_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERALL PROMPTS NEWLY ADDED\n",
    "DATA_TASK_IDS = (\n",
    "    \"data_cleaning\",\n",
    "    \"data_analytics\",\n",
    "    \"data_business_insights\",\n",
    "    \"data_model_cycle\"\n",
    ")\n",
    "\n",
    "# DATA CLEANING PARENT & CHILD STAGES\n",
    "_DATA_CLEANING_META = (\n",
    "    (\"data_brief\", \"Summarize, document, identify and understand the dataset and data columns.\"),\n",
    "    (\"missing_data\", \"Identify and process the missing Data Values existing per data field\"),\n",
    "    (\"anomality_detection\", \"Identify and process anomality\"),\n",
    "    (\"data_transform\", \"Transforming data columns, for example, encoding binary labels.\")\n",
    ")\n",
    "_MISSING_DATA_META = (\n",
    "   (\"define_missing_type\", \"Distinguish type of missing data values: MCAR, MNAR or MAR.\"),\n",
    "   (\"missing_report\", \"Summarize Missing Reports for each Data Column\")\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# Missing Data Stage Config\n",
    "# ====================================================\n",
    "MISSING_DATA_CONFIG = {\n",
    "    \"distinguish_mcar_mar\": {\n",
    "        \"prompt\": (\n",
    "            \"Stage: Distinguish MCAR vs MAR\\n\\n\"\n",
    "            \"Hypotheses:\\n\"\n",
    "            \"- H0: Missingness is completely at random (MCAR).\\n\"\n",
    "            \"- H1: Missingness depends on observed variables (MAR).\\n\\n\"\n",
    "            \"Dataset Preview:\\n{dataset_preview}\\n\\n\"\n",
    "            \"Instruction:\\n\"\n",
    "            \"Test whether missingness in '{target_col}' is independent of observed variables (MCAR) \"\n",
    "            \"or dependent on observed variables (MAR).\\n\\n\"\n",
    "            \"Respond only with the most appropriate tool name from the following list:\\n\"\n",
    "            \"{tools}\"\n",
    "        ),\n",
    "        \"input_template\": (\n",
    "            \"Please analyze missingness for '{target_col}' against '{features}' or '{group_col}' \"\n",
    "            \"using chi-square, logistic regression, or random forest feature importance.\"\n",
    "        ),\n",
    "        \"tools\": [\n",
    "            \"littles_mcar_test\",\n",
    "            \"chi_square_missingness\",\n",
    "            \"test_uniform_missing_multilabel\",\n",
    "            \"logistic_regression_missingness\",\n",
    "            \"random_forest_importance\"\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    \"distinguish_mar_mnar\": {\n",
    "        \"prompt\": (\n",
    "            \"Stage: Distinguish MAR vs MNAR\\n\\n\"\n",
    "            \"Hypotheses:\\n\"\n",
    "            \"- H0: Missingness is at random (MAR).\\n\"\n",
    "            \"- H1: Missingness depends on the unobserved/missing value itself (MNAR).\\n\\n\"\n",
    "            \"Dataset Preview:\\n{dataset_preview}\\n\\n\"\n",
    "            \"Instruction:\\n\"\n",
    "            \"Test whether missingness in '{target_col}' is fully explained by observed variables (MAR) \"\n",
    "            \"or depends on unobserved values (MNAR).\\n\\n\"\n",
    "            \"Respond only with the most appropriate tool name from the following list:\\n\"\n",
    "            \"{tools}\"\n",
    "        ),\n",
    "        \"input_template\": (\n",
    "            \"Please analyze missingness for '{target_col}' using Heckman selection models \"\n",
    "            \"or sensitivity analysis (extremes/bounds).\"\n",
    "        ),\n",
    "        \"tools\": [\n",
    "            \"heckman_selection\",\n",
    "            \"sensitivity_analysis\"\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95051be5",
   "metadata": {},
   "source": [
    "#### Mock Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac4868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MOCK_PROMPT = \"\"\"You are a reasoning agent for missing data classification.\n",
    "Task:\n",
    "Given the dataset field summary (table of data columns, their data types, and missing value ratios) and the available diagnostic tools, your job is to choose the most appropriate action (tool) to test whether the missingness of a given target column is best explained as:\n",
    "\n",
    "- MCAR (Missing Completely At Random)\n",
    "- MAR (Missing At Random)\n",
    "- MNAR (Missing Not At Random)\n",
    "\n",
    "Definitions:\n",
    "- MCAR: Missingness is completely random, unrelated to observed or unobserved variables.\n",
    "- MAR: Missingness depends only on observed variables (e.g., age, gender).\n",
    "- MNAR: Missingness depends on the missing/unobserved value itself (e.g., high income not reported).\n",
    "\n",
    "Dataset Field Summary (example):\n",
    "| Column Name | Data Type     | Missing Ratio |\n",
    "|-------------|---------------|---------------|\n",
    "| age         | numeric       | 0.02          |\n",
    "| income      | numeric       | 0.15          |\n",
    "| gender      | categorical   | 0.01          |\n",
    "| region      | categorical   | 0.00          |\n",
    "\n",
    "Target Column:\n",
    "'age'\n",
    "\n",
    "Available Tools:\n",
    "- littles_mcar_test: Correlation among missingness indicators, proxy for Littleâ€™s MCAR test.\n",
    "- chi_square_missingness: Test missingness in target_col against a group_col using chi-square.\n",
    "- test_uniform_missing_multilabel: Goodness-of-fit for uniform missing across labels.\n",
    "- logistic_regression_missingness: Logistic regression of missingness ~ observed covariates.\n",
    "- random_forest_importance: Predict missingness using observed covariates with feature importances.\n",
    "- heckman_selection: Selection model to test dependence on unobserved values (MNAR suspicion).\n",
    "- sensitivity_analysis: Impute with extremes/bounds to test MNAR robustness.\n",
    "\n",
    "Instruction:\n",
    "1. Carefully review the dataset field summary, target column, and tool descriptions.\n",
    "2. Identify whether the missingness for the target column should be tested under MCAR, MAR, or MNAR conditions.\n",
    "3. Respond **only with the single most appropriate tool name** from the provided list that should be applied first.\n",
    "\n",
    "Output Format:\n",
    "Return only the tool name (string), no reasoning or explanation.\"\"\"\n",
    "\n",
    "TOOL_CALLER_TRANSCRIPT = \"\"\"You are a reasoning agent for missing data classification.\n",
    "Task:\n",
    "Given the dataset field summary (table of data columns, their data types, and missing value ratios) and the available diagnostic tools, your job is to choose the most appropriate action (tool) to test whether the missingness of a given target column is best explained as:\n",
    "{input_task_description}\n",
    "\n",
    "Dataset Field Summary:\n",
    "{input_data_field_summary}\n",
    "\n",
    "Target Column:\n",
    "{input_target_col}\n",
    "\n",
    "Available Tools:\n",
    "{input_tools_list}\n",
    "\n",
    "Instruction:\n",
    "{input_instruction}\n",
    "\n",
    "Output Format:\n",
    "Return only the tool name (string), no reasoning or explanation.\"\"\"\n",
    "\n",
    "TOOL_CALLER_MISSING_TARGET = \"\"\"You are a reasoning agent for missing data classification.\n",
    "Task:\n",
    "Given the dataset field summary (table of data columns, their data types, and missing value ratios) and the available diagnostic tools, your job is to choose the most appropriate action (tool) to test whether the missingness of a given target column is best explained as:\n",
    "\n",
    "- MCAR (Missing Completely At Random)\n",
    "- MAR (Missing At Random)\n",
    "- MNAR (Missing Not At Random)\n",
    "\n",
    "Definitions:\n",
    "- MCAR: Missingness is completely random, unrelated to observed or unobserved variables.\n",
    "- MAR: Missingness depends only on observed variables (e.g., age, gender).\n",
    "- MNAR: Missingness depends on the missing/unobserved value itself (e.g., high income not reported).\n",
    "\n",
    "Dataset Field Summary (example):\n",
    "{input_data_field_summary}\n",
    "\n",
    "Target Column:\n",
    "{input_target_col}\n",
    "\n",
    "Available Tools:\n",
    "- littles_mcar_test: Correlation among missingness indicators, proxy for Littleâ€™s MCAR test.\n",
    "- chi_square_missingness: Test missingness in target_col against a group_col using chi-square.\n",
    "- test_uniform_missing_multilabel: Goodness-of-fit for uniform missing across labels.\n",
    "- logistic_regression_missingness: Logistic regression of missingness ~ observed covariates.\n",
    "- random_forest_importance: Predict missingness using observed covariates with feature importances.\n",
    "- heckman_selection: Selection model to test dependence on unobserved values (MNAR suspicion).\n",
    "- sensitivity_analysis: Impute with extremes/bounds to test MNAR robustness.\n",
    "\n",
    "Instruction:\n",
    "1. Carefully review the dataset field summary, target column, and tool descriptions.\n",
    "2. Identify whether the missingness for the target column should be tested under MCAR, MAR, or MNAR conditions.\n",
    "3. Respond **only with the single most appropriate tool name** from the provided list that should be applied first.\n",
    "\n",
    "Output Format:\n",
    "Return only the tool name (string), no reasoning or explanation.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead2a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISSING DATA STAGE CONFIGURATION & PROMPTS\n",
    "\n",
    "# Missing Data Classification Agent\n",
    "MISSING_DATA_CLASSIFIER_PROMPT = \"\"\"Â \n",
    "You are a Data Science Project manager for a data cleaning workflow. Given the current project's workflow, task objectives, data field summary defining all data columns in the dataset, your task is to decide on the next stage to action for your team.\n",
    "\n",
    "# Project Task Description\n",
    "\n",
    "The project task is to distinguish the missing dataset to one of the following labels:\n",
    "\n",
    "- MCAR (Missing Completely At Random):\n",
    "  Missingness occurs entirely by chance and is unrelated to both observed and unobserved variables.\n",
    "  Example: survey responses lost due to a server glitch, affecting everyone equally.\n",
    "\n",
    "- MAR (Missing At Random):\n",
    "  Missingness depends only on observed variables, not on the missing values themselves.\n",
    "  Example: older participants are less likely to answer a tech question, but age is recorded in the dataset.\n",
    "\n",
    "- MNAR (Missing Not At Random):\n",
    "  Missingness depends directly on the unobserved/missing value itself, even after accounting for observed variables.\n",
    "  Example: people with very high income choose not to report their salary, specifically because of its value.\n",
    "\n",
    "# Project Procedure Steps\n",
    "\n",
    "1. Describe the testing method suitable with the datatype of the given data column type.\n",
    "2. If MCAR is rejected, test for MAR. Ensure the test aligns with the data column and data types.\n",
    "3. Only classify as MNAR if residual dependence suggests missingness depends on unobserved values.\n",
    "5. End with the classification label: MCAR, MAR, or MNAR.\n",
    "\n",
    "# Current Project Step\n",
    "\n",
    "Data Field Summary:\n",
    "{data_field_summary}\n",
    "\n",
    "Target Data Field Name\n",
    "{target_column}\n",
    "\n",
    "# Previous History Actions\n",
    "\n",
    "Previous Actions\n",
    "{action_history_summary}\n",
    "\n",
    "Given the information above, respond the next stage to action from one of the listed steps under Project Procedure.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#AVAILABLE_FUNCTIONS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1719d533",
   "metadata": {},
   "source": [
    "### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6522e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24763854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ollama\n",
    "\n",
    "CHAT_CONFIG = dict(\n",
    "    stream=False,\n",
    "    # think='low',\n",
    "    options=DEFAULT_OPTIONS,\n",
    "    keep_alive='15m',\n",
    "    # tools = FUNCTION CALLABLES\n",
    ")\n",
    "\n",
    "class OllamaBasement:\n",
    "    \"\"\"\n",
    "    Minimal decorator for wrapping functions that return prompts/messages.\n",
    "    Only supports chat mode (no generate).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str, prompt: Instructor, **kwargs: dict):\n",
    "        self.model_id = model\n",
    "        self.prompt = prompt\n",
    "        self.kwargs = CHAT_CONFIG if not kwargs else kwargs\n",
    "\n",
    "    @property\n",
    "    def system_prompt(self):# -> list[dict[str, Any]]:\n",
    "        text = self.prompt.prompt if isinstance(self.prompt, Instructor) else str(self.prompt)\n",
    "        return [{\"role\": \"system\", \"content\": text}]\n",
    "\n",
    "    def __call__(self, fn: Callable[...]):\n",
    "        def wrapper(*args, **kwargs) -> str:\n",
    "            try:\n",
    "                inputs: str = fn(*args, **kwargs)\n",
    "                message = self.system_prompt + [{\"role\": \"user\", \"content\": inputs}]\n",
    "                client = ollama.Client(LIGHTNING_OLLAMA_HOST_URL)\n",
    "                response = client.chat(model=self.model_id, messages=message, **self.kwargs)\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9ccfc48b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__new__() takes exactly one argument (the type to instantiate)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m writer \u001b[38;5;241m=\u001b[39m \u001b[43mSummarizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproject_stages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData Cleaning Pipeline: 1) Data Loading, 2) Missing Value Detection, 3) Outlier Analysis, 4) Data Validation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mimeus-app/backend/gaby/src/gaby_agent/core/agent/_core.py:42\u001b[0m, in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     host_url \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIGHTNING_HOST_OLLAMA_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIGHTNING_OLLAMA_HOST_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01melse\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOLLAMA_HOST_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instance\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mClient(host_url)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to init GabyBasement client: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: object.__new__() takes exactly one argument (the type to instantiate)"
     ]
    }
   ],
   "source": [
    "writer = Summarizer(\n",
    "            project_stages=\"Data Cleaning Pipeline: 1) Data Loading, 2) Missing Value Detection, 3) Outlier Analysis, 4) Data Validation\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
